{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nkj5fyDiZaEq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import random\n",
        "import argparse\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn import set_config\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import gensim\n",
        "import string\n",
        "import sys\n",
        "from huggingface_hub import HfApi, HfFolder, upload_folder\n",
        "from google.colab import userdata\n",
        "import joblib\n",
        "\n",
        "# Ki·ªÉm tra v√† c√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
        "try:\n",
        "    import emoji\n",
        "except ModuleNotFoundError:\n",
        "    print(\"Installing emoji...\")\n",
        "    os.system(\"pip install emoji\")\n",
        "    import emoji\n",
        "try:\n",
        "    import wordninja\n",
        "except ModuleNotFoundError:\n",
        "    print(\"Installing wordninja...\")\n",
        "    os.system(\"pip install wordninja\")\n",
        "    import wordninja\n",
        "try:\n",
        "    import nlpaug.augmenter.word as naw\n",
        "except ModuleNotFoundError:\n",
        "    print(\"Installing nlpaug...\")\n",
        "    os.system(\"pip install nlpaug\")\n",
        "    import nlpaug.augmenter.word as naw\n",
        "try:\n",
        "    import gensim\n",
        "except ModuleNotFoundError:\n",
        "    print(\"Installing gensim...\")\n",
        "    os.system(\"pip install gensim\")\n",
        "    import gensim\n",
        "try:\n",
        "    import xgboost\n",
        "except ModuleNotFoundError:\n",
        "    print(\"Installing xgboost...\")\n",
        "    os.system(\"pip install xgboost\")\n",
        "    import xgboost\n",
        "\n",
        "# Ki·ªÉm tra phi√™n b·∫£n scikit-learn\n",
        "import sklearn\n",
        "print(f\"scikit-learn version: {sklearn.__version__}\")\n",
        "print(f\"scikit-learn path: {sklearn.__file__}\")\n",
        "if sklearn.__version__ < '1.3':\n",
        "    raise ImportError(\"scikit-learn version 1.3 or higher is required. Please upgrade using: !pip install --force-reinstall scikit-learn>=1.3\")\n",
        "\n",
        "# Thi·∫øt l·∫≠p c·∫•u h√¨nh scikit-learn\n",
        "set_config(enable_metadata_routing=True)\n",
        "\n",
        "# Thi·∫øt l·∫≠p logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ƒê·ªãnh nghƒ©a c√°c √°nh x·∫° cho ti·ªÅn x·ª≠ l√Ω\n",
        "contraction_mapping = {\n",
        "    \"ain't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"'cause\": \"because\",\n",
        "    \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",\n",
        "    \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
        "    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\n",
        "    \"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
        "    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "    \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\",\n",
        "    \"I'll've\": \"I will have\", \"I'm\": \"I am\", \"I've\": \"I have\",\n",
        "    \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",\n",
        "    \"i'll've\": \"i will have\", \"i'm\": \"i am\", \"i've\": \"i have\",\n",
        "    \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\",\n",
        "    \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\",\n",
        "    \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\",\n",
        "    \"might've\": \"might have\", \"mightn't\": \"might not\",\n",
        "    \"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "    \"mustn't\": \"must not\", \"mustn't've\": \"must not have\",\n",
        "    \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
        "    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\",\n",
        "    \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
        "    \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "    \"she'd\": \"she would\", \"she'd've\": \"she would have\",\n",
        "    \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "    \"should've\": \"should have\", \"shouldn't\": \"should not\",\n",
        "    \"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so as\",\n",
        "    \"this's\": \"this is\", \"that'd\": \"that would\", \"that'd've\": \"that would have\",\n",
        "    \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\",\n",
        "    \"there's\": \"there is\", \"here's\": \"here is\", \"they'd\": \"they would\",\n",
        "    \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n",
        "    \"they'll've\": \"they will have\", \"they're\": \"they are\",\n",
        "    \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\",\n",
        "    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\",\n",
        "    \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n",
        "    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\",\n",
        "    \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\",\n",
        "    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\",\n",
        "    \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\",\n",
        "    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "    \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\",\n",
        "    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\",\n",
        "    \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "    \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\",\n",
        "    \"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\",\n",
        "    \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "    \"you're\": \"you are\", \"you've\": \"you have\", \"u.s\": \"america\", \"e.g\": \"for example\"\n",
        "}\n",
        "\n",
        "punct_mapping = {\n",
        "    \"‚Äò\": \"'\", \"‚Çπ\": \"e\", \"¬¥\": \"'\", \"¬∞\": \"\", \"‚Ç¨\": \"e\", \"‚Ñ¢\": \"tm\", \"‚àö\": \" sqrt \",\n",
        "    \"√ó\": \"x\", \"¬≤\": \"2\", \"‚Äî\": \"-\", \"‚Äì\": \"-\", \"‚Äô\": \"'\", \"_\": \"-\", \"`\": \"'\", '‚Äú': '\"',\n",
        "    '‚Äù': '\"', '‚Äú': '\"', \"¬£\": \"e\", '‚àû': 'infinity', 'Œ∏': 'theta', '√∑': '/', 'Œ±': 'alpha',\n",
        "    '‚Ä¢': '.', '√†': 'a', '‚àí': '-', 'Œ≤': 'beta', '‚àÖ': '', '¬≥': '3', 'œÄ': 'pi'\n",
        "}\n",
        "\n",
        "mispell_dict = {\n",
        "    'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling',\n",
        "    'counselling': 'counseling', 'theatre': 'theater', 'cancelled': \"canceled\", 'labour': 'labor',\n",
        "    'organisation': \"organization\", 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ',\n",
        "    'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do',\n",
        "    'whatare': 'what are', 'howcan': \"how can\", 'howmuch': 'how much', 'howmany': 'how many',\n",
        "    'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does',\n",
        "    'mastrubation': 'masturbation', 'mastrubate': 'masturbate', 'mastrubating': 'masturbating',\n",
        "    'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data',\n",
        "    '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend',\n",
        "    'airhostess': 'air hostess', 'whst': 'what', 'watsapp': 'whatsapp',\n",
        "    'demonitisation': 'demonetization', 'demonitization': 'demonetization',\n",
        "    'demonetisation': \"demonetization\", 'pissed': 'angry', 'overthinking': 'overthinking'\n",
        "}\n",
        "\n",
        "punct_chars = list((set(string.punctuation) | {\n",
        "    \"‚Äô\", \"‚Äò\", \"‚Äì\", \"‚Äî\", \"~\", \"|\", \"‚Äú\", \"‚Äù\", \"‚Ä¶\", \"'\", \"`\", \"_\", \"‚Äú\"\n",
        "}) - set([\"#\", \"!\", \"?\"]))\n",
        "punct_chars.sort()\n",
        "punctuation = \"\".join(punct_chars)\n",
        "replace = re.compile(\"[%s]\" % re.escape(punctuation))\n",
        "\n",
        "# H√†m t·∫£i GloVe v√† emoji2vec\n",
        "def load_embeddings(glove_path, emoji2vec_path, embedding_dim, logger):\n",
        "    logger.info(f\"ƒêang t·∫£i GloVe t·ª´ {glove_path}\")\n",
        "    glove_embeddings = {}\n",
        "    try:\n",
        "        with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                values = line.split()\n",
        "                word = values[0]\n",
        "                vector = np.asarray(values[1:], dtype='float32')\n",
        "                glove_embeddings[word] = vector\n",
        "        logger.info(f\"ƒê√£ t·∫£i {len(glove_embeddings)} vector GloVe\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"L·ªói khi t·∫£i GloVe t·ª´ {glove_path}: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "    logger.info(f\"ƒêang t·∫£i emoji2vec t·ª´ {emoji2vec_path}\")\n",
        "    emoji2vec = None\n",
        "    try:\n",
        "        with open(emoji2vec_path, 'r', encoding='utf-8') as f:\n",
        "            first_line = f.readline()\n",
        "            if not first_line.strip():\n",
        "                raise ValueError(f\"T·ªáp emoji2vec {emoji2vec_path} r·ªóng\")\n",
        "        emoji2vec = gensim.models.KeyedVectors.load_word2vec_format(\n",
        "            emoji2vec_path, binary=False, unicode_errors='ignore'\n",
        "        )\n",
        "        logger.info(f\"ƒê√£ t·∫£i {len(emoji2vec.key_to_index)} vector emoji\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"L·ªói khi t·∫£i emoji2vec t·ª´ {emoji2vec_path}: {str(e)}\")\n",
        "        raise\n",
        "    return glove_embeddings, emoji2vec\n",
        "\n",
        "# H√†m t·∫°o ƒë·∫∑c tr∆∞ng t·ª´ vƒÉn b·∫£n v·ªõi tr·ªçng s·ªë TF-IDF\n",
        "def create_features(texts, tokenizer, glove_embeddings, emoji2vec, embedding_dim, max_len, logger):\n",
        "    tfidf = TfidfVectorizer(max_features=30000)\n",
        "    tfidf.fit(texts)\n",
        "    tfidf_weights = tfidf.transform(texts).toarray()\n",
        "    word_to_tfidf = {word: tfidf.idf_[idx] for word, idx in tfidf.vocabulary_.items()}\n",
        "\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    padded = pad_sequences(sequences, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
        "    features = np.zeros((len(texts), embedding_dim), dtype=np.float32)\n",
        "\n",
        "    for i, seq in enumerate(padded):\n",
        "        vectors = []\n",
        "        weights = []\n",
        "        for idx in seq:\n",
        "            if idx == 0:  # Padding token\n",
        "                continue\n",
        "            word = tokenizer.index_word.get(idx, \"<OOV>\")\n",
        "            weight = word_to_tfidf.get(word, 1.0)\n",
        "            if emoji.is_emoji(word) and word in emoji2vec:\n",
        "                vectors.append(emoji2vec[word])\n",
        "                weights.append(weight)\n",
        "            elif word in glove_embeddings:\n",
        "                vectors.append(glove_embeddings[word])\n",
        "                weights.append(weight)\n",
        "        if vectors:\n",
        "            weights = np.array(weights)\n",
        "            vectors = np.array(vectors)\n",
        "            weighted_avg = np.average(vectors, axis=0, weights=weights / np.sum(weights))\n",
        "            features[i] = weighted_avg\n",
        "        else:\n",
        "            features[i] = np.zeros(embedding_dim)\n",
        "    logger.info(f\"ƒê√£ t·∫°o ƒë·∫∑c tr∆∞ng cho {len(texts)} m·∫´u, k√≠ch th∆∞·ªõc: {features.shape}\")\n",
        "    return features\n",
        "\n",
        "# H√†m ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n\n",
        "def clean_text(text, logger):\n",
        "    if not isinstance(text, str) or not text:\n",
        "        return \"\"\n",
        "    logger.debug(f\"VƒÉn b·∫£n g·ªëc: {text}\")\n",
        "\n",
        "    for contraction, full_form in contraction_mapping.items():\n",
        "        text = text.replace(contraction, full_form)\n",
        "\n",
        "    for p, replacement in punct_mapping.items():\n",
        "        text = text.replace(p, replacement)\n",
        "\n",
        "    def split_hashtag(match):\n",
        "        hashtag = match.group(0)[1:]\n",
        "        words = wordninja.split(hashtag)\n",
        "        return ' '.join(words)\n",
        "    text = re.sub(r\"#\\w+\", split_hashtag, text)\n",
        "\n",
        "    text = re.sub(r\"http\\S*|\\S*\\.com\\S*|\\S*www\\S*\", \" \", text)\n",
        "    text = re.sub(r\"\\s@\\S+\", \" \", text)\n",
        "\n",
        "    text = replace.sub(\" \", text)\n",
        "\n",
        "    text = text.lower()\n",
        "    words = text.split()\n",
        "    words = [mispell_dict.get(word, word) for word in words]\n",
        "    text = ' '.join(words)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    logger.debug(f\"VƒÉn b·∫£n ƒë√£ x·ª≠ l√Ω: {text}\")\n",
        "    return text\n",
        "\n",
        "# L·ªõp x·ª≠ l√Ω d·ªØ li·ªáu GoEmotions\n",
        "class GoemotionsProcessor:\n",
        "    def __init__(self, args, logger):\n",
        "        self.args = args\n",
        "        self.logger = logger\n",
        "\n",
        "    def get_labels(self):\n",
        "        label_file = os.path.join(self.args.data_dir, self.args.label_file)\n",
        "        self.logger.info(f\"ƒêang ƒë·ªçc t·ªáp nh√£n t·∫°i: {label_file}\")\n",
        "        if not os.path.exists(label_file):\n",
        "            self.logger.warning(f\"Kh√¥ng t√¨m th·∫•y t·ªáp nh√£n t·∫°i {label_file}. S·ª≠ d·ª•ng nh√£n m·∫∑c ƒë·ªãnh GoEmotions.\")\n",
        "            labels = [\n",
        "                'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion',\n",
        "                'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment',\n",
        "                'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism',\n",
        "                'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral'\n",
        "            ]\n",
        "            self.logger.info(f\"S·ª≠ d·ª•ng {len(labels)} nh√£n m·∫∑c ƒë·ªãnh\")\n",
        "            return labels\n",
        "        try:\n",
        "            with open(label_file, \"r\", encoding=\"utf-8\") as f:\n",
        "                labels = [line.strip() for line in f if line.strip()]\n",
        "            if not labels:\n",
        "                self.logger.error(f\"T·ªáp nh√£n {label_file} r·ªóng\")\n",
        "                raise ValueError(f\"T·ªáp nh√£n {label_file} r·ªóng\")\n",
        "            self.logger.info(f\"ƒê√£ ƒë·ªçc {len(labels)} nh√£n t·ª´ {label_file}\")\n",
        "            return labels\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"L·ªói khi ƒë·ªçc t·ªáp nh√£n {label_file}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _read_file(self, input_file):\n",
        "        if not os.path.exists(input_file):\n",
        "            self.logger.error(f\"Kh√¥ng t√¨m th·∫•y t·ªáp d·ªØ li·ªáu t·∫°i {input_file}\")\n",
        "            raise FileNotFoundError(f\"Kh√¥ng t√¨m th·∫•y t·ªáp d·ªØ li·ªáu t·∫°i {input_file}\")\n",
        "        try:\n",
        "            df = pd.read_csv(input_file, sep='\\t', header=None, names=['text', 'labels', 'id'])\n",
        "            self.logger.info(f\"ƒê√£ ƒë·ªçc {len(df)} d√≤ng t·ª´ {input_file}\")\n",
        "            if df.empty:\n",
        "                self.logger.error(f\"T·ªáp d·ªØ li·ªáu {input_file} r·ªóng\")\n",
        "                raise ValueError(f\"T·ªáp d·ªØ li·ªáu {input_file} r·ªóng\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"L·ªói khi ƒë·ªçc t·ªáp {input_file}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _augment_data(self, texts, labels):\n",
        "        aug = naw.SynonymAug(aug_p=0.3)\n",
        "        augmented_texts, augmented_labels = [], []\n",
        "        for text, label in zip(texts, labels):\n",
        "            augmented_texts.append(text)\n",
        "            augmented_labels.append(label)\n",
        "            aug_text = aug.augment(text)[0]\n",
        "            augmented_texts.append(aug_text)\n",
        "            augmented_labels.append(label)\n",
        "        self.logger.info(f\"S·ªë m·∫´u sau tƒÉng c∆∞·ªùng: {len(augmented_texts)}\")\n",
        "        return augmented_texts, augmented_labels\n",
        "\n",
        "    def _balance_labels(self, examples, label_list_len, set_type):\n",
        "        if set_type != \"train\":\n",
        "            return examples\n",
        "        self.logger.info(\"C√¢n b·∫±ng nh√£n cho t·∫≠p hu·∫•n luy·ªán\")\n",
        "        label_counts = Counter()\n",
        "        for ex in examples:\n",
        "            label_counts.update(ex['labels'])\n",
        "        self.logger.info(f\"Ph√¢n b·ªë nh√£n ban ƒë·∫ßu: {dict(label_counts)}\")\n",
        "        counts = [count for count in label_counts.values() if count > 0]\n",
        "        target_count = min(int(np.median(counts) * 6.0), len(examples) // 2)\n",
        "        self.logger.info(f\"S·ªë l∆∞·ª£ng m·ª•c ti√™u m·ªói nh√£n: {target_count}\")\n",
        "        balanced_examples = []\n",
        "        for label in range(label_list_len):\n",
        "            samples_with_label = [ex for ex in examples if label in ex['labels']]\n",
        "            current_count = label_counts.get(label, 0)\n",
        "            if current_count == 0:\n",
        "                continue\n",
        "            elif current_count < target_count:\n",
        "                samples_needed = target_count - current_count\n",
        "                oversampled = random.choices(samples_with_label, k=samples_needed)\n",
        "                balanced_examples.extend(oversampled)\n",
        "            else:\n",
        "                samples_to_keep = max(target_count, int(current_count * 0.5))\n",
        "                balanced_examples.extend(random.sample(samples_with_label, min(samples_to_keep, len(samples_with_label))))\n",
        "        balanced_examples.extend([ex for ex in examples if not any(label in ex['labels'] for label in range(label_list_len))])\n",
        "        random.shuffle(balanced_examples)\n",
        "        new_label_counts = Counter()\n",
        "        for ex in balanced_examples:\n",
        "            new_label_counts.update(ex['labels'])\n",
        "        self.logger.info(f\"Ph√¢n b·ªë nh√£n sau c√¢n b·∫±ng: {dict(new_label_counts)}\")\n",
        "        self.logger.info(f\"S·ªë m·∫´u ban ƒë·∫ßu: {len(examples)}, S·ªë m·∫´u sau c√¢n b·∫±ng: {len(balanced_examples)}\")\n",
        "        return balanced_examples\n",
        "\n",
        "    def get_examples(self, mode):\n",
        "        file_map = {\n",
        "            'train': self.args.train_file,\n",
        "            'dev': self.args.dev_file,\n",
        "            'test': self.args.test_file\n",
        "        }\n",
        "        file_to_read = file_map.get(mode)\n",
        "        if not file_to_read:\n",
        "            raise ValueError(\"Mode ph·∫£i l√† 'train', 'dev', ho·∫∑c 'test'\")\n",
        "        file_path = os.path.join(self.args.data_dir, file_to_read)\n",
        "        self.logger.info(f\"ƒêang ƒë·ªçc d·ªØ li·ªáu {mode} t·ª´ {file_path}\")\n",
        "        df = self._read_file(file_path)\n",
        "        return self._create_examples(df, mode)\n",
        "\n",
        "    def _create_examples(self, df, set_type):\n",
        "        examples = []\n",
        "        label_list_len = len(self.get_labels())\n",
        "        label_counts = Counter()\n",
        "        for i, row in df.iterrows():\n",
        "            guid = f\"{set_type}-{i}\"\n",
        "            raw_text = row['text']\n",
        "            label_str = str(row['labels'])\n",
        "            try:\n",
        "                label = [int(l) for l in label_str.split(',') if l.strip().isdigit()]\n",
        "                label = [l for l in label if 0 <= l < label_list_len]\n",
        "                if not label:\n",
        "                    self.logger.warning(f\"Kh√¥ng c√≥ nh√£n h·ª£p l·ªá t·∫°i d√≤ng {i}: {label_str}. B·ªè qua.\")\n",
        "                    continue\n",
        "                label_counts.update(label)\n",
        "            except (ValueError, IndexError) as e:\n",
        "                self.logger.warning(f\"Nh√£n kh√¥ng h·ª£p l·ªá t·∫°i d√≤ng {i}: {label_str}. B·ªè qua. L·ªói: {e}\")\n",
        "                continue\n",
        "            cleaned_text = clean_text(raw_text, self.logger)\n",
        "            examples.append({\n",
        "                'guid': guid,\n",
        "                'text': cleaned_text,\n",
        "                'labels': label\n",
        "            })\n",
        "        self.logger.info(f\"ƒê√£ t·∫°o {len(examples)} m·∫´u t·ª´ {set_type}\")\n",
        "        self.logger.info(f\"Ph√¢n b·ªë nh√£n cho {set_type} (tr∆∞·ªõc c√¢n b·∫±ng): {dict(label_counts)}\")\n",
        "        if not examples:\n",
        "            self.logger.error(f\"Kh√¥ng t·∫°o ƒë∆∞·ª£c m·∫´u t·ª´ {set_type}. Ki·ªÉm tra t·ªáp d·ªØ li·ªáu!\")\n",
        "            raise ValueError(f\"Kh√¥ng t·∫°o ƒë∆∞·ª£c m·∫´u t·ª´ {set_type}\")\n",
        "        examples = self._balance_labels(examples, label_list_len, set_type)\n",
        "        return examples\n",
        "\n",
        "# H√†m t·∫£i d·ªØ li·ªáu\n",
        "def load_data(args, logger):\n",
        "    processor = GoemotionsProcessor(args, logger)\n",
        "    label_list = processor.get_labels()\n",
        "    num_labels = len(label_list)\n",
        "    def load_and_cache(mode):\n",
        "        cached_file = os.path.join(args.data_dir, f\"cached_{mode}_data.pkl\")\n",
        "        if os.path.exists(cached_file):\n",
        "            logger.info(f\"ƒêang t·∫£i d·ªØ li·ªáu ƒë√£ cache t·ª´ {cached_file}\")\n",
        "            with open(cached_file, 'rb') as f:\n",
        "                data = pickle.load(f)\n",
        "            return data['texts'], data['labels']\n",
        "        logger.info(f\"ƒêang x·ª≠ l√Ω d·ªØ li·ªáu {mode}\")\n",
        "        examples = processor.get_examples(mode)\n",
        "        texts = [ex['text'] for ex in examples]\n",
        "        labels = [ex['labels'] for ex in examples]\n",
        "        if mode == 'train':\n",
        "            texts, labels = processor._augment_data(texts, labels)\n",
        "        with open(cached_file, 'wb') as f:\n",
        "            pickle.dump({'texts': texts, 'labels': labels}, f)\n",
        "        logger.info(f\"ƒê√£ cache d·ªØ li·ªáu {mode} v√†o {cached_file}\")\n",
        "        return texts, labels\n",
        "    try:\n",
        "        train_texts, train_labels = load_and_cache('train')\n",
        "        val_texts, val_labels = load_and_cache('dev')\n",
        "        test_texts, test_labels = load_and_cache('test')\n",
        "        logger.info(f\"S·ªë m·∫´u hu·∫•n luy·ªán: {len(train_texts)}, S·ªë m·∫´u x√°c th·ª±c: {len(val_texts)}, S·ªë m·∫´u ki·ªÉm tra: {len(test_texts)}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"L·ªói khi t·∫£i d·ªØ li·ªáu: {str(e)}\")\n",
        "        raise\n",
        "    return train_texts, val_texts, test_texts, train_labels, val_labels, test_labels, label_list\n",
        "\n",
        "# H√†m m√£ h√≥a nh√£n th√†nh d·∫°ng multi-hot\n",
        "def to_multi_hot(label_lists, num_labels):\n",
        "    m = np.zeros((len(label_lists), num_labels), dtype=np.int32)\n",
        "    for i, labs in enumerate(label_lists):\n",
        "        m[i, labs] = 1\n",
        "    return m\n",
        "\n",
        "# H√†m t√≠nh tr·ªçng s·ªë l·ªõp\n",
        "def compute_class_weights(labels, num_labels, logger):\n",
        "    label_counts = np.zeros(num_labels)\n",
        "    for labs in labels:\n",
        "        for l in labs:\n",
        "            label_counts[l] += 1\n",
        "\n",
        "    epsilon = 1e-8\n",
        "    label_counts = np.maximum(label_counts, epsilon)\n",
        "\n",
        "    total_samples = len(labels)\n",
        "    class_weights = {}\n",
        "    for i in range(num_labels):\n",
        "        if label_counts[i] > 0:\n",
        "            class_weights[i] = total_samples / (num_labels * label_counts[i])\n",
        "            class_weights[i] = max(class_weights[i], 1.0)\n",
        "        else:\n",
        "            class_weights[i] = 1.0\n",
        "\n",
        "    logger.info(f\"Ph√¢n b·ªë nh√£n: {label_counts}\")\n",
        "    logger.info(f\"Tr·ªçng s·ªë l·ªõp: {class_weights}\")\n",
        "    return class_weights\n",
        "\n",
        "# H√†m t√¨m ng∆∞·ª°ng t·ªëi ∆∞u tr√™n t·∫≠p x√°c th·ª±c\n",
        "def find_optimal_thresholds(model, X_val, y_val, num_labels, logger):\n",
        "    pred_probs = model.predict_proba(X_val)\n",
        "    thresholds = np.arange(0.1, 0.5, 0.05)\n",
        "    best_thresholds = np.ones(num_labels) * 0.25  # Gi√° tr·ªã m·∫∑c ƒë·ªãnh\n",
        "    best_f1 = 0.0\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        y_pred = (pred_probs >= threshold).astype(int)\n",
        "        micro_f1 = f1_score(y_val, y_pred, average='micro', zero_division=0)\n",
        "        if micro_f1 > best_f1:\n",
        "            best_f1 = micro_f1\n",
        "            best_thresholds = np.ones(num_labels) * threshold\n",
        "\n",
        "    logger.info(f\"Ng∆∞·ª°ng t·ªëi ∆∞u: {best_thresholds}, Micro F1: {best_f1:.4f}\")\n",
        "    return best_thresholds\n",
        "\n",
        "# H√†m d·ª± ƒëo√°n cho c√°c v√≠ d·ª• m·ªõi\n",
        "def predict_examples(model, tokenizer, examples, label_list, glove_embeddings, emoji2vec, embedding_dim, max_len, thresholds, logger):\n",
        "    print(\"\\n===== D·ª± ƒëo√°n cho c√°c v√≠ d·ª• ki·ªÉm tra m·ªõi =====\")\n",
        "    if not examples:\n",
        "        print(\"Kh√¥ng c√≥ v√≠ d·ª• ki·ªÉm tra n√†o ƒë∆∞·ª£c cung c·∫•p!\")\n",
        "        return []\n",
        "    cleaned_examples = [clean_text(ex, logger) for ex in examples]\n",
        "    print(\"VƒÉn b·∫£n sau khi ti·ªÅn x·ª≠ l√Ω:\", cleaned_examples)\n",
        "    features = create_features(cleaned_examples, tokenizer, glove_embeddings, emoji2vec, embedding_dim, max_len, logger)\n",
        "    pred_probs = model.predict_proba(features)\n",
        "    predictions = np.zeros_like(pred_probs, dtype=int)\n",
        "    for i in range(len(label_list)):\n",
        "        predictions[:, i] = (pred_probs[:, i] >= thresholds[i]).astype(int)\n",
        "\n",
        "    results = []\n",
        "    for i, example in enumerate(examples):\n",
        "        predicted_labels = [label_list[j] for j in range(len(label_list)) if predictions[i][j] == 1]\n",
        "        top_indices = np.argsort(pred_probs[i])[-3:][::-1]\n",
        "        top_labels = [label_list[j] for j in top_indices]\n",
        "        top_probs = [pred_probs[i][j] for j in top_indices]\n",
        "        print(f\"\\nV√≠ d·ª• {i+1}: {example}\")\n",
        "        print(f\"D·ª± ƒëo√°n (ng∆∞·ª°ng t√πy ch·ªânh): {predicted_labels if predicted_labels else 'Kh√¥ng c√≥ nh√£n'}\")\n",
        "        print(f\"Top-3 nh√£n: {list(zip(top_labels, top_probs))}\")\n",
        "        results.append({\n",
        "            \"text\": example,\n",
        "            \"labels\": predicted_labels if predicted_labels else \"Kh√¥ng c√≥ nh√£n\",\n",
        "            \"top_labels\": list(zip(top_labels, top_probs)),\n",
        "            \"probs\": pred_probs[i].tolist()\n",
        "        })\n",
        "    return results\n",
        "\n",
        "# H√†m ch√≠nh\n",
        "def main():\n",
        "    # C·∫•u h√¨nh tham s·ªë\n",
        "    parser = argparse.ArgumentParser(description=\"XGBoost Multi-Label Classification for GoEmotions or VAAFI\")\n",
        "    parser.add_argument(\"--data_dir\", default=\"/content/drive/MyDrive/Goemotions/data\", type=str, help=\"Th∆∞ m·ª•c ch·ª©a d·ªØ li·ªáu\")\n",
        "    parser.add_argument(\"--train_file\", default=\"train.tsv\", type=str, help=\"T·ªáp d·ªØ li·ªáu hu·∫•n luy·ªán\")\n",
        "    parser.add_argument(\"--dev_file\", default=\"dev.tsv\", type=str, help=\"T·ªáp d·ªØ li·ªáu x√°c th·ª±c\")\n",
        "    parser.add_argument(\"--test_file\", default=\"test.tsv\", type=str, help=\"T·ªáp d·ªØ li·ªáu ki·ªÉm tra\")\n",
        "    parser.add_argument(\"--label_file\", default=\"labels.txt\", type=str, help=\"T·ªáp ch·ª©a danh s√°ch nh√£n\")\n",
        "    parser.add_argument(\"--glove_path\", default=\"/content/drive/MyDrive/Goemotions/glove.6B.300d.txt\", type=str, help=\"ƒê∆∞·ªùng d·∫´n ƒë·∫øn t·ªáp GloVe\")\n",
        "    parser.add_argument(\"--emoji2vec_path\", default=\"/content/drive/MyDrive/Goemotions/emoji2vec.txt\", type=str, help=\"ƒê∆∞·ªùng d·∫´n ƒë·∫øn t·ªáp emoji2vec\")\n",
        "    parser.add_argument(\"--ckpt_dir\", default=\"/content/drive/MyDrive/Goemotions/checkpoints\", type=str, help=\"Th∆∞ m·ª•c l∆∞u checkpoint m√¥ h√¨nh\")\n",
        "    parser.add_argument(\"--model_type\", default=\"goemotions-xgb\", type=str, help=\"Lo·∫°i m√¥ h√¨nh (goemotions-xgb ho·∫∑c vaafi-xgb)\")\n",
        "    parser.add_argument(\"--hf_repo_id\", default=\"Songnguyen263/{model_type}\", type=str, help=\"ID kho l∆∞u tr·ªØ Hugging Face (username/repo_name), d√πng {model_type} ƒë·ªÉ thay th·∫ø\")\n",
        "\n",
        "    args = parser.parse_args([arg for arg in sys.argv[1:] if not arg.startswith('-f') and not arg.endswith('.json')])\n",
        "\n",
        "    # Thay th·∫ø {model_type} trong hf_repo_id\n",
        "    args.hf_repo_id = args.hf_repo_id.format(model_type=args.model_type)\n",
        "\n",
        "    # Mount Google Drive\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        print(\"Google Drive mounted successfully\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error mounting Google Drive: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "    # T·∫°o th∆∞ m·ª•c checkpoint\n",
        "    try:\n",
        "        os.makedirs(args.ckpt_dir, exist_ok=True)\n",
        "        logger.info(f\"Th∆∞ m·ª•c checkpoint: {args.ckpt_dir}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"L·ªói khi t·∫°o th∆∞ m·ª•c checkpoint: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "    # T·∫£i d·ªØ li·ªáu\n",
        "    train_texts, val_texts, test_texts, train_labels, val_labels, test_labels, label_list = load_data(args, logger)\n",
        "    num_labels = len(label_list)\n",
        "\n",
        "    # M√£ h√≥a nh√£n\n",
        "    y_train = to_multi_hot(train_labels, num_labels)\n",
        "    y_val = to_multi_hot(val_labels, num_labels)\n",
        "    y_test = to_multi_hot(test_labels, num_labels)\n",
        "\n",
        "    # Token h√≥a v√† t·∫°o ƒë·∫∑c tr∆∞ng\n",
        "    vocab_size = 30000\n",
        "    max_len = 100\n",
        "    embedding_dim = 300\n",
        "\n",
        "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\", filters='!\"$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "    tokenizer.fit_on_texts(train_texts + ['pissed', 'overthinking'])  # Th√™m t·ª´ kh√≥a quan tr·ªçng\n",
        "\n",
        "    try:\n",
        "        with open(os.path.join(args.ckpt_dir, f\"{args.model_type}_tokenizer.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(tokenizer, f)\n",
        "        logger.info(f\"ƒê√£ l∆∞u tokenizer v√†o {args.model_type}_tokenizer.pkl\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"L·ªói khi l∆∞u tokenizer: {str(e)}\")\n",
        "        raise\n",
        "    logger.info(f\"T·ª´ ƒëi·ªÉn tokenizer: {dict(list(tokenizer.word_index.items())[:50])}\")\n",
        "    logger.info(f\"'angry' trong t·ª´ ƒëi·ªÉn: {'angry' in tokenizer.word_index}\")\n",
        "    logger.info(f\"'overthinking' trong t·ª´ ƒëi·ªÉn: {'overthinking' in tokenizer.word_index}\")\n",
        "\n",
        "    # T·∫£i v√† t·∫°o ƒë·∫∑c tr∆∞ng t·ª´ embeddings\n",
        "    glove_embeddings, emoji2vec = load_embeddings(args.glove_path, args.emoji2vec_path, embedding_dim, logger)\n",
        "    X_train = create_features(train_texts, tokenizer, glove_embeddings, emoji2vec, embedding_dim, max_len, logger)\n",
        "    X_val = create_features(val_texts, tokenizer, glove_embeddings, emoji2vec, embedding_dim, max_len, logger)\n",
        "    X_test = create_features(test_texts, tokenizer, glove_embeddings, emoji2vec, embedding_dim, max_len, logger)\n",
        "\n",
        "    # X√¢y d·ª±ng m√¥ h√¨nh XGBoost\n",
        "    class_weights = compute_class_weights(train_labels, num_labels, logger)\n",
        "    model = OneVsRestClassifier(\n",
        "        XGBClassifier(\n",
        "            max_depth=6,\n",
        "            learning_rate=0.1,\n",
        "            n_estimators=200,  # TƒÉng s·ªë l∆∞·ª£ng estimators\n",
        "            objective='binary:logistic',\n",
        "            eval_metric='logloss',\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi early stopping\n",
        "    try:\n",
        "        logger.info(\"ƒêang hu·∫•n luy·ªán m√¥ h√¨nh XGBoost v·ªõi early stopping...\")\n",
        "        model.fit(\n",
        "            X_train, y_train,\n",
        "            estimator__eval_set=[(X_val, y_val)],\n",
        "            estimator__early_stopping_rounds=10,\n",
        "            estimator__verbose=False\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Early stopping kh√¥ng kh·∫£ d·ª•ng: {str(e)}. S·ª≠ d·ª•ng fit th√¥ng th∆∞·ªùng.\")\n",
        "        model.fit(X_train, y_train)\n",
        "    logger.info(\"Hu·∫•n luy·ªán ho√†n t·∫•t\")\n",
        "\n",
        "    # T√¨m ng∆∞·ª°ng t·ªëi ∆∞u tr√™n t·∫≠p x√°c th·ª±c\n",
        "    thresholds = find_optimal_thresholds(model, X_val, y_val, num_labels, logger)\n",
        "\n",
        "    # ƒê√°nh gi√° m√¥ h√¨nh\n",
        "    pred_probs = model.predict_proba(X_test)\n",
        "    y_pred = np.zeros_like(pred_probs, dtype=int)\n",
        "    for i in range(num_labels):\n",
        "        y_pred[:, i] = (pred_probs[:, i] >= thresholds[i]).astype(int)\n",
        "\n",
        "    # B√°o c√°o k·∫øt qu·∫£\n",
        "    raw_acc = accuracy_score(y_test.flatten(), y_pred.flatten())\n",
        "    micro_f1 = f1_score(y_test, y_pred, average='micro', zero_division=0)\n",
        "    macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "    weighted_f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    report = classification_report(y_test, y_pred, target_names=label_list, zero_division=0, digits=4)\n",
        "\n",
        "    logger.info(f\"Raw Accuracy (sklearn, element-wise): {raw_acc:.4f}\")\n",
        "    logger.info(f\"Micro-averaged F1 score: {micro_f1:.4f}\")\n",
        "    logger.info(f\"Macro-averaged F1 score: {macro_f1:.4f}\")\n",
        "    logger.info(f\"Weighted-averaged F1 score: {weighted_f1:.4f}\")\n",
        "    logger.info(f\"\\nClassification Report (custom thresholds):\\n{report}\")\n",
        "\n",
        "    # Ph√¢n t√≠ch l·ªói\n",
        "    errors = []\n",
        "    for i, (true, pred, text) in enumerate(zip(y_test, y_pred, test_texts)):\n",
        "        if not np.array_equal(true, pred):\n",
        "            errors.append((text, true, pred))\n",
        "    logger.info(f\"C√°c m·∫´u d·ª± ƒëo√°n sai (top 10): {errors[:10]}\")\n",
        "\n",
        "    # V√≠ d·ª• ki·ªÉm tra m·ªõi\n",
        "    test_examples = [\n",
        "        \"Feeling on top of the world today! üéâüòä #BestDayEver #SoHappy\",\n",
        "        \"Totally let down... üò¢üíî #Disappointed #WhyThis\",\n",
        "        \"Omg that‚Äôs incredible news! üòç‚ú® #Amazing #Grateful\",\n",
        "        \"Head full of thoughts rn ü§Øü§î #Confused #Overthinking\",\n",
        "        \"Still can‚Äôt believe this happened. So pissed! üò°üî• #Angry #Unbelievable\"\n",
        "    ]\n",
        "    results = predict_examples(model, tokenizer, test_examples, label_list, glove_embeddings, emoji2vec, embedding_dim, max_len, thresholds, logger)\n",
        "\n",
        "    # L∆∞u k·∫øt qu·∫£ v√† m√¥ h√¨nh\n",
        "    output_dir = \"/content/drive/MyDrive/Goemotions/\"\n",
        "    try:\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        output_file = os.path.join(output_dir, \"XGB-results.txt\")\n",
        "\n",
        "        with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"\\n=== XGBoost Results - {args.model_type} - {pd.Timestamp.now()} ===\\n\")\n",
        "            f.write(f\"Raw Accuracy (sklearn, element-wise): {raw_acc:.4f}\\n\")\n",
        "            f.write(f\"Micro-averaged F1 score: {micro_f1:.4f}\\n\")\n",
        "            f.write(f\"Macro-averaged F1 score: {macro_f1:.4f}\\n\")\n",
        "            f.write(f\"Weighted-averaged F1 score: {weighted_f1:.4f}\\n\")\n",
        "            f.write(f\"\\nClassification Report (custom thresholds):\\n{report}\\n\")\n",
        "            f.write(\"\\nExample Predictions:\\n\")\n",
        "            for i, result in enumerate(results):\n",
        "                f.write(f\"Example {i+1}: {result['text']}\\n\")\n",
        "                f.write(f\"Predicted Labels: {result['labels']}\\n\")\n",
        "                f.write(f\"Top-3 Labels and Probabilities: {result['top_labels']}\\n\\n\")\n",
        "            f.write(\"================================\\n\")\n",
        "        logger.info(f\"ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o {output_file}\")\n",
        "\n",
        "        # L∆∞u m√¥ h√¨nh\n",
        "        model_file = os.path.join(args.ckpt_dir, f\"{args.model_type}_model.pkl\")\n",
        "        joblib.dump(model, model_file)\n",
        "        logger.info(f\"ƒê√£ l∆∞u m√¥ h√¨nh v√†o {model_file}\")\n",
        "\n",
        "        # L∆∞u ng∆∞·ª°ng\n",
        "        threshold_file = os.path.join(args.ckpt_dir, f\"{args.model_type}_thresholds.pkl\")\n",
        "        with open(threshold_file, 'wb') as f:\n",
        "            pickle.dump(thresholds, f)\n",
        "        logger.info(f\"ƒê√£ l∆∞u ng∆∞·ª°ng v√†o {threshold_file}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"L·ªói khi l∆∞u k·∫øt qu·∫£ ho·∫∑c m√¥ h√¨nh: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}