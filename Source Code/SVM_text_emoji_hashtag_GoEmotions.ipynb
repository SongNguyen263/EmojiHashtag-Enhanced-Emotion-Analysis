{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Om3r6wtLZJsG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import random\n",
        "import argparse\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import gensim\n",
        "import string\n",
        "import sys\n",
        "from huggingface_hub import HfApi, HfFolder, upload_folder\n",
        "from google.colab import userdata\n",
        "import joblib\n",
        "import emoji\n",
        "import wordninja\n",
        "import nlpaug.augmenter.word as naw\n",
        "import json\n",
        "import time\n",
        "\n",
        "# Thi·∫øt l·∫≠p logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ƒê·ªãnh nghƒ©a c√°c √°nh x·∫° cho ti·ªÅn x·ª≠ l√Ω\n",
        "CONTRACTION_MAPPING = {\n",
        "    \"ain't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"'cause\": \"because\",\n",
        "    \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",\n",
        "    \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
        "    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\n",
        "    \"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
        "    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "    \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\",\n",
        "    \"I'll've\": \"I will have\", \"I'm\": \"I am\", \"I've\": \"I have\",\n",
        "    \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",\n",
        "    \"i'll've\": \"i will have\", \"i'm\": \"i am\", \"i've\": \"i have\",\n",
        "    \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\",\n",
        "    \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\",\n",
        "    \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\",\n",
        "    \"might've\": \"might have\", \"mightn't\": \"might not\",\n",
        "    \"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "    \"mustn't\": \"must not\", \"mustn't've\": \"must not have\",\n",
        "    \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
        "    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\",\n",
        "    \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
        "    \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "    \"she'd\": \"she would\", \"she'd've\": \"she would have\",\n",
        "    \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "    \"should've\": \"should have\", \"shouldn't\": \"should not\",\n",
        "    \"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so as\",\n",
        "    \"this's\": \"this is\", \"that'd\": \"that would\", \"that'd've\": \"that would have\",\n",
        "    \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\",\n",
        "    \"there's\": \"there is\", \"here's\": \"here is\", \"they'd\": \"they would\",\n",
        "    \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n",
        "    \"they'll've\": \"they will have\", \"they're\": \"they are\",\n",
        "    \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\",\n",
        "    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\",\n",
        "    \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n",
        "    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\",\n",
        "    \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\",\n",
        "    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\",\n",
        "    \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\",\n",
        "    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "    \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\",\n",
        "    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\",\n",
        "    \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "    \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\",\n",
        "    \"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\",\n",
        "    \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "    \"you're\": \"you are\", \"you've\": \"you have\", \"u.s\": \"america\", \"e.g\": \"for example\"\n",
        "}\n",
        "\n",
        "PUNCT_MAPPING = {\n",
        "    \"‚Äò\": \"'\", \"‚Çπ\": \"e\", \"¬¥\": \"'\", \"¬∞\": \"\", \"‚Ç¨\": \"e\", \"‚Ñ¢\": \"tm\", \"‚àö\": \" sqrt \",\n",
        "    \"√ó\": \"x\", \"¬≤\": \"2\", \"‚Äî\": \"-\", \"‚Äì\": \"-\", \"‚Äô\": \"'\", \"_\": \"-\", \"`\": \"'\", '‚Äú': '\"',\n",
        "    '‚Äù': '\"', '‚Äú': '\"', \"¬£\": \"e\", '‚àû': 'infinity', 'Œ∏': 'theta', '√∑': '/', 'Œ±': 'alpha',\n",
        "    '‚Ä¢': '.', '√†': 'a', '‚àí': '-', 'Œ≤': 'beta', '‚àÖ': '', '¬≥': '3', 'œÄ': 'pi'\n",
        "}\n",
        "\n",
        "MISPELL_DICT = {\n",
        "    'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling',\n",
        "    'counselling': 'counseling', 'theatre': 'theater', 'cancelled': \"canceled\", 'labour': 'labor',\n",
        "    'organisation': \"organization\", 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ',\n",
        "    'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do',\n",
        "    'whatare': 'what are', 'howcan': \"how can\", 'howmuch': 'how much', 'howmany': 'how many',\n",
        "    'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does',\n",
        "    'mastrubation': 'masturbation', 'mastrubate': 'masturbate', 'mastrubating': 'masturbating',\n",
        "    'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data',\n",
        "    '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend',\n",
        "    'airhostess': 'air hostess', 'whst': 'what', 'watsapp': 'whatsapp',\n",
        "    'demonitisation': 'demonetization', 'demonitization': 'demonetization',\n",
        "    'demonetisation': \"demonetization\", 'pissed': 'pissed'\n",
        "}\n",
        "\n",
        "PUNCT_CHARS = list((set(string.punctuation) | {\n",
        "    \"‚Äô\", \"‚Äò\", \"‚Äì\", \"‚Äî\", \"~\", \"|\", \"‚Äú\", \"‚Äù\", \"‚Ä¶\", \"'\", \"`\", \"_\", \"‚Äú\"\n",
        "}) - set([\"#\", \"!\", \"?\", \"üòä\", \"üéâ\", \"üò¢\", \"üíî\", \"üòç\", \"‚ú®\", \"ü§Ø\", \"ü§î\", \"üò°\", \"üî•\"]))\n",
        "PUNCT_CHARS.sort()\n",
        "PUNCTUATION = \"\".join(PUNCT_CHARS)\n",
        "REPLACE_PUNCT = re.compile(\"[%s]\" % re.escape(PUNCTUATION))\n",
        "\n",
        "def load_embeddings(glove_path: str, emoji2vec_path: str, embedding_dim: int, logger: logging.Logger) -> tuple:\n",
        "    logger.info(f\"Loading GloVe from {glove_path}\")\n",
        "    glove_embeddings = {}\n",
        "    try:\n",
        "        with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                values = line.strip().split()\n",
        "                if len(values) < embedding_dim + 1:\n",
        "                    continue\n",
        "                word = values[0]\n",
        "                vector = np.asarray(values[1:], dtype='float32')\n",
        "                glove_embeddings[word] = vector\n",
        "        logger.info(f\"Loaded {len(glove_embeddings)} GloVe vectors\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading GloVe from {glove_path}: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "    logger.info(f\"Loading emoji2vec from {emoji2vec_path}\")\n",
        "    try:\n",
        "        emoji2vec = gensim.models.KeyedVectors.load_word2vec_format(\n",
        "            emoji2vec_path, binary=False, unicode_errors='ignore'\n",
        "        )\n",
        "        logger.info(f\"Loaded {len(emoji2vec.key_to_index)} emoji vectors\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading emoji2vec from {emoji2vec_path}: {str(e)}\")\n",
        "        raise\n",
        "    return glove_embeddings, emoji2vec\n",
        "\n",
        "def clean_text(text: str, logger: logging.Logger) -> str:\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return \"\"\n",
        "\n",
        "    for contraction, full_form in CONTRACTION_MAPPING.items():\n",
        "        text = text.replace(contraction, full_form)\n",
        "\n",
        "    for p, replacement in PUNCT_MAPPING.items():\n",
        "        text = text.replace(p, replacement)\n",
        "\n",
        "    def split_hashtag(match):\n",
        "        hashtag = match.group(0)[1:]\n",
        "        words = wordninja.split(hashtag)\n",
        "        return ' '.join(words)\n",
        "    text = re.sub(r\"#\\w+\", split_hashtag, text)\n",
        "\n",
        "    text = re.sub(r\"http\\S*|\\S*\\.com\\S*|\\S*www\\S*|\\s@\\S+\", \" \", text)\n",
        "    text = REPLACE_PUNCT.sub(\" \", text)\n",
        "\n",
        "    text = text.lower()\n",
        "    words = text.split()\n",
        "    words = [MISPELL_DICT.get(word, word) for word in words]\n",
        "    text = ' '.join(words)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def create_features(texts: list, tokenizer: Tokenizer, glove_embeddings: dict, emoji2vec: gensim.models.KeyedVectors,\n",
        "                   embedding_dim: int, max_len: int, logger: logging.Logger) -> np.ndarray:\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    padded = pad_sequences(sequences, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
        "    features = np.zeros((len(texts), embedding_dim), dtype=np.float32)\n",
        "\n",
        "    for i, seq in enumerate(padded):\n",
        "        vectors = []\n",
        "        for idx in seq:\n",
        "            if idx == 0:\n",
        "                continue\n",
        "            word = tokenizer.index_word.get(idx, \"<OOV>\")\n",
        "            if emoji.is_emoji(word) and word in emoji2vec:\n",
        "                vectors.append(emoji2vec[word])\n",
        "            elif word in glove_embeddings:\n",
        "                vectors.append(glove_embeddings[word])\n",
        "        if vectors:\n",
        "            features[i] = np.mean(vectors, axis=0)\n",
        "\n",
        "    logger.info(f\"Created features for {len(texts)} samples, shape: {features.shape}\")\n",
        "    return features\n",
        "\n",
        "class GoemotionsProcessor:\n",
        "    def __init__(self, args, logger):\n",
        "        self.args = args\n",
        "        self.logger = logger\n",
        "\n",
        "    def get_labels(self) -> list:\n",
        "        label_file = os.path.join(self.args.data_dir, self.args.label_file)\n",
        "        self.logger.info(f\"Reading labels from {label_file}\")\n",
        "        if not os.path.exists(label_file):\n",
        "            self.logger.warning(f\"Label file {label_file} not found. Using default GoEmotions labels.\")\n",
        "            labels = [\n",
        "                'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion',\n",
        "                'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment',\n",
        "                'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism',\n",
        "                'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral'\n",
        "            ]\n",
        "            return labels\n",
        "        with open(label_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            labels = [line.strip() for line in f if line.strip()]\n",
        "        if not labels:\n",
        "            self.logger.error(f\"Label file {label_file} is empty\")\n",
        "            raise ValueError(f\"Label file {label_file} is empty\")\n",
        "        self.logger.info(f\"Loaded {len(labels)} labels\")\n",
        "        return labels\n",
        "\n",
        "    def _read_file(self, input_file: str) -> pd.DataFrame:\n",
        "        if not os.path.exists(input_file):\n",
        "            self.logger.error(f\"Data file {input_file} not found\")\n",
        "            raise FileNotFoundError(f\"Data file {input_file} not found\")\n",
        "        try:\n",
        "            df = pd.read_csv(input_file, sep='\\t', header=None, names=['text', 'labels', 'id'])\n",
        "            if df.empty:\n",
        "                self.logger.error(f\"Data file {input_file} is empty\")\n",
        "                raise ValueError(f\"Data file {input_file} is empty\")\n",
        "            self.logger.info(f\"Read {len(df)} lines from {input_file}\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error reading file {input_file}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _augment_data(self, texts: list, labels: list) -> tuple:\n",
        "        aug = naw.SynonymAug(aug_p=0.3)\n",
        "        augmented_texts, augmented_labels = [], []\n",
        "        for text, label in zip(texts, labels):\n",
        "            augmented_texts.append(text)\n",
        "            augmented_labels.append(label)\n",
        "            aug_text = aug.augment(text)[0]\n",
        "            augmented_texts.append(aug_text)\n",
        "            augmented_labels.append(label)\n",
        "        self.logger.info(f\"Augmented to {len(augmented_texts)} samples\")\n",
        "        return augmented_texts, augmented_labels\n",
        "\n",
        "    def _balance_labels(self, examples: list, label_list_len: int, set_type: str) -> list:\n",
        "        if set_type != \"train\":\n",
        "            return examples\n",
        "        self.logger.info(\"Balancing labels for training set\")\n",
        "        label_counts = Counter()\n",
        "        for ex in examples:\n",
        "            label_counts.update(ex['labels'])\n",
        "\n",
        "        counts = [count for count in label_counts.values() if count > 0]\n",
        "        target_count = min(int(np.median(counts) * 6.0), len(examples) // 2)\n",
        "        balanced_examples = []\n",
        "        for label in range(label_list_len):\n",
        "            samples_with_label = [ex for ex in examples if label in ex['labels']]\n",
        "            current_count = label_counts.get(label, 0)\n",
        "            if current_count == 0:\n",
        "                continue\n",
        "            elif current_count < target_count:\n",
        "                samples_needed = target_count - current_count\n",
        "                oversampled = random.choices(samples_with_label, k=samples_needed)\n",
        "                balanced_examples.extend(oversampled)\n",
        "            else:\n",
        "                samples_to_keep = max(target_count, int(current_count * 0.5))\n",
        "                balanced_examples.extend(random.sample(samples_with_label, min(samples_to_keep, len(samples_with_label))))\n",
        "        balanced_examples.extend([ex for ex in examples if not any(label in ex['labels'] for label in range(label_list_len))])\n",
        "        random.shuffle(balanced_examples)\n",
        "        self.logger.info(f\"Balanced from {len(examples)} to {len(balanced_examples)} samples\")\n",
        "        return balanced_examples\n",
        "\n",
        "    def get_examples(self, mode: str) -> list:\n",
        "        file_map = {'train': self.args.train_file, 'dev': self.args.dev_file, 'test': self.args.test_file}\n",
        "        file_to_read = file_map.get(mode)\n",
        "        if not file_to_read:\n",
        "            raise ValueError(\"Mode must be 'train', 'dev', or 'test'\")\n",
        "        file_path = os.path.join(self.args.data_dir, file_to_read)\n",
        "        df = self._read_file(file_path)\n",
        "        return self._create_examples(df, mode)\n",
        "\n",
        "    def _create_examples(self, df: pd.DataFrame, set_type: str) -> list:\n",
        "        examples = []\n",
        "        label_list_len = len(self.get_labels())\n",
        "        label_counts = Counter()\n",
        "        for i, row in df.iterrows():\n",
        "            guid = f\"{set_type}-{i}\"\n",
        "            raw_text = row['text']\n",
        "            label_str = str(row['labels'])\n",
        "            try:\n",
        "                label = [int(l) for l in label_str.split(',') if l.strip().isdigit()]\n",
        "                label = [l for l in label if 0 <= l < label_list_len]\n",
        "                if not label:\n",
        "                    self.logger.warning(f\"No valid labels at line {i} (text: {raw_text}, labels: {label_str}). Skipping.\")\n",
        "                    continue\n",
        "                label_counts.update(label)\n",
        "            except (ValueError, IndexError) as e:\n",
        "                self.logger.warning(f\"Invalid labels at line {i} (text: {raw_text}, labels: {label_str}). Error: {e}. Skipping.\")\n",
        "                continue\n",
        "            cleaned_text = clean_text(raw_text, self.logger)\n",
        "            if not cleaned_text:\n",
        "                self.logger.warning(f\"Empty text after cleaning at line {i} (original: {raw_text}). Skipping.\")\n",
        "                continue\n",
        "            examples.append({'guid': guid, 'text': cleaned_text, 'labels': label})\n",
        "\n",
        "        if not examples:\n",
        "            self.logger.error(f\"No valid examples created from {set_type}\")\n",
        "            raise ValueError(f\"No valid examples created from {set_type}\")\n",
        "\n",
        "        self.logger.info(f\"Created {len(examples)} examples from {set_type}\")\n",
        "        self.logger.info(f\"Label distribution for {set_type} (before balancing): {dict(label_counts)}\")\n",
        "        return self._balance_labels(examples, label_list_len, set_type)\n",
        "\n",
        "def load_data(args, logger: logging.Logger) -> tuple:\n",
        "    processor = GoemotionsProcessor(args, logger)\n",
        "    label_list = processor.get_labels()\n",
        "    def load_and_cache(mode: str) -> tuple:\n",
        "        cached_file = os.path.join(args.data_dir, f\"cached_{mode}_data.pkl\")\n",
        "        if os.path.exists(cached_file):\n",
        "            with open(cached_file, 'rb') as f:\n",
        "                data = pickle.load(f)\n",
        "            return data['texts'], data['labels']\n",
        "        examples = processor.get_examples(mode)\n",
        "        texts = [ex['text'] for ex in examples]\n",
        "        labels = [ex['labels'] for ex in examples]\n",
        "        if mode == 'train' and args.max_train_samples:\n",
        "            indices = random.sample(range(len(texts)), min(args.max_train_samples, len(texts)))\n",
        "            texts = [texts[i] for i in indices]\n",
        "            labels = [labels[i] for i in indices]\n",
        "        if mode == 'train':\n",
        "            texts, labels = processor._augment_data(texts, labels)\n",
        "        with open(cached_file, 'wb') as f:\n",
        "            pickle.dump({'texts': texts, 'labels': labels}, f)\n",
        "        return texts, labels\n",
        "    train_texts, train_labels = load_and_cache('train')\n",
        "    val_texts, val_labels = load_and_cache('dev')\n",
        "    test_texts, test_labels = load_and_cache('test')\n",
        "    return train_texts, val_texts, test_texts, train_labels, val_labels, test_labels, label_list\n",
        "\n",
        "def to_multi_hot(label_lists: list, num_labels: int, logger: logging.Logger) -> np.ndarray:\n",
        "    m = np.zeros((len(label_lists), num_labels), dtype=np.int32)\n",
        "    for i, labs in enumerate(label_lists):\n",
        "        valid_labs = []\n",
        "        for lab in labs:\n",
        "            try:\n",
        "                lab_int = int(lab)\n",
        "                if 0 <= lab_int < num_labels:\n",
        "                    valid_labs.append(lab_int)\n",
        "                else:\n",
        "                    logger.warning(f\"Invalid label index {lab_int} at sample {i} (out of range [0, {num_labels-1}]). Skipping.\")\n",
        "            except (ValueError, TypeError):\n",
        "                logger.warning(f\"Non-integer label {lab} at sample {i}. Skipping.\")\n",
        "        if not valid_labs:\n",
        "            logger.warning(f\"No valid labels for sample {i}. Using empty label set.\")\n",
        "            continue\n",
        "        m[i, valid_labs] = 1\n",
        "    return m\n",
        "\n",
        "def compute_class_weights(labels: list, num_labels: int, logger: logging.Logger) -> dict:\n",
        "    label_counts = np.zeros(num_labels)\n",
        "    for labs in labels:\n",
        "        for l in labs:\n",
        "            if 0 <= l < num_labels:\n",
        "                label_counts[l] += 1\n",
        "    epsilon = 1e-8\n",
        "    label_counts = np.maximum(label_counts, epsilon)\n",
        "    total_samples = len(labels)\n",
        "    class_weights = {}\n",
        "    median_count = np.median(label_counts)\n",
        "    for i in range(num_labels):\n",
        "        class_weights[i] = np.log(total_samples / label_counts[i]) if label_counts[i] > 0 else 1.0\n",
        "        if label_counts[i] < median_count:\n",
        "            class_weights[i] *= 2.0\n",
        "        class_weights[i] = max(class_weights[i], 1.0)\n",
        "    weight_sum = sum(class_weights.values())\n",
        "    if weight_sum > 0:\n",
        "        scale_factor = num_labels / weight_sum\n",
        "        for i in range(num_labels):\n",
        "            class_weights[i] *= scale_factor\n",
        "    return class_weights\n",
        "\n",
        "def predict_examples(model, tokenizer: Tokenizer, examples: list, label_list: list, glove_embeddings: dict,\n",
        "                    emoji2vec: gensim.models.KeyedVectors, embedding_dim: int, max_len: int, logger: logging.Logger) -> list:\n",
        "    if not examples:\n",
        "        logger.warning(\"No test examples provided\")\n",
        "        return []\n",
        "\n",
        "    cleaned_examples = [clean_text(ex, logger) for ex in examples]\n",
        "    logger.info(f\"Preprocessed texts: {cleaned_examples}\")\n",
        "    features = create_features(cleaned_examples, tokenizer, glove_embeddings, emoji2vec, embedding_dim, max_len, logger)\n",
        "    pred_probs = model.decision_function(features)\n",
        "\n",
        "    thresholds = np.percentile(pred_probs, 75, axis=1)\n",
        "    predictions = np.array([pred_probs[i] >= thresholds[i] for i in range(len(pred_probs))]).astype(int)\n",
        "\n",
        "    results = []\n",
        "    for i, example in enumerate(examples):\n",
        "        predicted_labels = [label_list[j] for j in range(len(label_list)) if predictions[i][j] == 1]\n",
        "        top_indices = np.argsort(pred_probs[i])[-3:][::-1]\n",
        "        top_labels = [label_list[j] for j in top_indices]\n",
        "        top_probs = [pred_probs[i][j] for j in top_indices]\n",
        "        print(f\"\\nExample {i+1}: {example}\")\n",
        "        print(f\"Predicted (dynamic threshold): {predicted_labels if predicted_labels else 'No label'}\")\n",
        "        print(f\"Top-3 labels: {list(zip(top_labels, top_probs))}\")\n",
        "        results.append({\n",
        "            \"text\": example,\n",
        "            \"labels\": predicted_labels if predicted_labels else \"No label\",\n",
        "            \"top_labels\": list(zip(top_labels, top_probs)),\n",
        "            \"probs\": pred_probs[i].tolist()\n",
        "        })\n",
        "    return results\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"SVM Multi-Label Classification for GoEmotions or VAAFI\")\n",
        "    parser.add_argument(\"--data_dir\", default=\"/content/drive/MyDrive/Goemotions/data\", type=str)\n",
        "    parser.add_argument(\"--train_file\", default=\"train0.tsv\", type=str)\n",
        "    parser.add_argument(\"--dev_file\", default=\"dev0.tsv\", type=str)\n",
        "    parser.add_argument(\"--test_file\", default=\"test0.tsv\", type=str)\n",
        "    parser.add_argument(\"--label_file\", default=\"labels.txt\", type=str)\n",
        "    parser.add_argument(\"--glove_path\", default=\"/content/drive/MyDrive/Goemotions/glove.6B.300d.txt\", type=str)\n",
        "    parser.add_argument(\"--emoji2vec_path\", default=\"/content/drive/MyDrive/Goemotions/emoji2vec.txt\", type=str)\n",
        "    parser.add_argument(\"--ckpt_dir\", default=\"/content/drive/MyDrive/Goemotions/checkpoints\", type=str)\n",
        "    parser.add_argument(\"--model_type\", default=\"goemotions-svm\", type=str)\n",
        "    parser.add_argument(\"--hf_repo_id\", default=\"Songnguyen263/{model_type}\", type=str)\n",
        "    parser.add_argument(\"--hf_token\", default=None, type=str, help=\"Hugging Face API token\")\n",
        "    parser.add_argument(\"--max_train_samples\", type=int, default=None, help=\"Maximum number of training samples to use\")\n",
        "    parser.add_argument(\"--max_iter\", type=int, default=500, help=\"Maximum iterations for SVM training\")\n",
        "\n",
        "    args = parser.parse_args([arg for arg in sys.argv[1:] if not arg.startswith('-f') and not arg.endswith('.json')])\n",
        "    args.hf_repo_id = args.hf_repo_id.format(model_type=args.model_type)\n",
        "\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    logger.info(\"Google Drive mounted successfully\")\n",
        "\n",
        "    os.makedirs(args.ckpt_dir, exist_ok=True)\n",
        "\n",
        "    train_texts, val_texts, test_texts, train_labels, val_labels, test_labels, label_list = load_data(args, logger)\n",
        "    num_labels = len(label_list)\n",
        "    logger.info(f\"Number of labels: {num_labels}\")\n",
        "    logger.info(f\"Training samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
        "\n",
        "    # Validate labels\n",
        "    for split, labels in [(\"train\", train_labels), (\"val\", val_labels), (\"test\", test_labels)]:\n",
        "        invalid_labels = [labs for labs in labels if any(not isinstance(lab, int) or lab < 0 or lab >= num_labels for lab in labs)]\n",
        "        if invalid_labels:\n",
        "            logger.error(f\"Found {len(invalid_labels)} invalid label sets in {split} split: {invalid_labels[:5]}\")\n",
        "            raise ValueError(f\"Invalid labels detected in {split} split\")\n",
        "\n",
        "    y_train = to_multi_hot(train_labels, num_labels, logger)\n",
        "    y_val = to_multi_hot(val_labels, num_labels, logger)\n",
        "    y_test = to_multi_hot(test_labels, num_labels, logger)\n",
        "\n",
        "    vocab_size = 30000\n",
        "    max_len = 100\n",
        "    embedding_dim = 300\n",
        "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\", filters='!\"$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "    tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "    with open(os.path.join(args.ckpt_dir, f\"{args.model_type}_tokenizer.pkl\"), \"wb\") as f:\n",
        "        pickle.dump(tokenizer, f)\n",
        "\n",
        "    glove_embeddings, emoji2vec = load_embeddings(args.glove_path, args.emoji2vec_path, embedding_dim, logger)\n",
        "    X_train = create_features(train_texts, tokenizer, glove_embeddings, emoji2vec, embedding_dim, max_len, logger)\n",
        "    X_val = create_features(val_texts, tokenizer, glove_embeddings, emoji2vec, embedding_dim, max_len, logger)\n",
        "    X_test = create_features(test_texts, tokenizer, glove_embeddings, emoji2vec, embedding_dim, max_len, logger)\n",
        "\n",
        "    class_weights = compute_class_weights(train_labels, num_labels, logger)\n",
        "    model = OneVsRestClassifier(LinearSVC(C=1.0, class_weight=class_weights, max_iter=args.max_iter, tol=1e-3))\n",
        "\n",
        "    logger.info(\"Starting SVM training...\")\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        model.fit(X_train, y_train)\n",
        "        logger.info(f\"Training completed in {time.time() - start_time:.2f} seconds\")\n",
        "    except KeyboardInterrupt:\n",
        "        logger.warning(\"Training interrupted. Saving partial model...\")\n",
        "        joblib.dump(model, os.path.join(args.ckpt_dir, f\"{args.model_type}_partial_model.joblib\"))\n",
        "        logger.info(f\"Partial model saved to {args.ckpt_dir}/{args.model_type}_partial_model.joblib\")\n",
        "        raise\n",
        "\n",
        "    pred_probs = model.decision_function(X_test)\n",
        "    thresholds = np.percentile(pred_probs, 75, axis=1)\n",
        "    y_pred = np.array([pred_probs[i] >= thresholds[i] for i in range(len(pred_probs))]).astype(int)\n",
        "\n",
        "    raw_acc = accuracy_score(y_test.flatten(), y_pred.flatten())\n",
        "    micro_f1 = f1_score(y_test, y_pred, average='micro', zero_division=0)\n",
        "    macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "    weighted_f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    report = classification_report(y_test, y_pred, target_names=label_list, zero_division=0, digits=4)\n",
        "\n",
        "    logger.info(f\"Raw Accuracy: {raw_acc:.4f}\")\n",
        "    logger.info(f\"Micro F1: {micro_f1:.4f}\")\n",
        "    logger.info(f\"Macro F1: {macro_f1:.4f}\")\n",
        "    logger.info(f\"Weighted F1: {weighted_f1:.4f}\")\n",
        "    logger.info(f\"\\nClassification Report:\\n{report}\")\n",
        "\n",
        "    test_examples = [\n",
        "        \"Feeling on top of the world today! üéâüòä #BestDayEver #SoHappy\",\n",
        "        \"Totally let down... üò¢üíî #Disappointed #WhyThis\",\n",
        "        \"Omg that‚Äôs incredible news! üòç‚ú® #Amazing #Grateful\",\n",
        "        \"Head full of thoughts rn ü§Øü§î #Confused #Overthinking\",\n",
        "        \"Still can‚Äôt believe this happened. So pissed! üò°üî• #Angry #Unbelievable\"\n",
        "    ]\n",
        "    predict_examples(model, tokenizer, test_examples, label_list, glove_embeddings, emoji2vec, embedding_dim, max_len, logger)\n",
        "\n",
        "    output_dir = \"/content/drive/MyDrive/Goemotions/\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_file = os.path.join(output_dir, \"SVM-results.txt\")\n",
        "    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"\\n=== SVM Results - {args.model_type} - {pd.Timestamp.now()} ===\\n\")\n",
        "        f.write(f\"Raw Accuracy: {raw_acc:.4f}\\n\")\n",
        "        f.write(f\"Micro F1: {micro_f1:.4f}\\n\")\n",
        "        f.write(f\"Macro F1: {macro_f1:.4f}\\n\")\n",
        "        f.write(f\"Weighted F1: {weighted_f1:.4f}\\n\")\n",
        "        f.write(f\"\\nClassification Report:\\n{report}\\n\")\n",
        "\n",
        "    local_model_dir = os.path.join(args.ckpt_dir, f\"hf_{args.model_type}\")\n",
        "    os.makedirs(local_model_dir, exist_ok=True)\n",
        "    joblib.dump(model, os.path.join(local_model_dir, f\"{args.model_type}_model.joblib\"))\n",
        "    with open(os.path.join(local_model_dir, f\"{args.model_type}_tokenizer.pkl\"), \"wb\") as f:\n",
        "        pickle.dump(tokenizer, f)\n",
        "    with open(os.path.join(local_model_dir, f\"{args.model_type}_labels.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        for label in label_list:\n",
        "            f.write(f\"{label}\\n\")\n",
        "    config = {\n",
        "        \"vocab_size\": vocab_size,\n",
        "        \"max_len\": max_len,\n",
        "        \"embedding_dim\": embedding_dim,\n",
        "        \"num_labels\": num_labels,\n",
        "        \"model_type\": args.model_type\n",
        "    }\n",
        "    with open(os.path.join(local_model_dir, f\"{args.model_type}_config.json\"), \"w\") as f:\n",
        "        json.dump(config, f, indent=4)\n",
        "\n",
        "    if args.hf_token:\n",
        "        HfFolder.save_token(args.hf_token)\n",
        "        api = HfApi()\n",
        "        api.upload_folder(\n",
        "            folder_path=local_model_dir,\n",
        "            repo_id=args.hf_repo_id,\n",
        "            repo_type=\"model\",\n",
        "            commit_message=f\"Upload {args.model_type} model and tokenizer\"\n",
        "        )\n",
        "        logger.info(f\"Uploaded model to Hugging Face: {args.hf_repo_id}\")\n",
        "    else:\n",
        "        logger.warning(\"No Hugging Face token provided. Skipping upload.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}