{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Set up Library"
      ],
      "metadata": {
        "id": "sb1VL3qNYK1a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8Q0ZOdnYEU9"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow>=2.10\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install scikit-learn\n",
        "!pip install gensim\n",
        "!pip install wordninja\n",
        "!pip install emoji\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ftfy nlpaug imbalanced-learn"
      ],
      "metadata": {
        "id": "obPatXpuYUkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-wGR9z-bYXwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model"
      ],
      "metadata": {
        "id": "oYoss3ZcYYdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import random\n",
        "import argparse\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.mixed_precision import set_global_policy\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, precision_recall_fscore_support\n",
        "import gensim\n",
        "import string\n",
        "import sys\n",
        "try:\n",
        "    import emoji\n",
        "except ModuleNotFoundError:\n",
        "    print(\"Installing emoji...\")\n",
        "    os.system(\"pip install emoji\")\n",
        "    import emoji\n",
        "try:\n",
        "    import wordninja\n",
        "except ModuleNotFoundError:\n",
        "    print(\"Installing wordninja...\")\n",
        "    os.system(\"pip install wordninja\")\n",
        "    import wordninja\n",
        "try:\n",
        "    import nlpaug.augmenter.word as naw\n",
        "except ModuleNotFoundError:\n",
        "    print(\"Installing nlpaug...\")\n",
        "    os.system(\"pip install nlpaug\")\n",
        "    import nlpaug.augmenter.word as naw\n",
        "try:\n",
        "    import gensim\n",
        "except ModuleNotFoundError:\n",
        "    print(\"Installing gensim...\")\n",
        "    os.system(\"pip install gensim\")\n",
        "    import gensim\n",
        "try:\n",
        "    import ftfy\n",
        "except ModuleNotFoundError:\n",
        "    print(\"Installing ftfy...\")\n",
        "    os.system(\"pip install ftfy\")\n",
        "    import ftfy\n",
        "\n",
        "# Thi·∫øt l·∫≠p logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ƒê·ªãnh nghƒ©a c√°c √°nh x·∫° cho ti·ªÅn x·ª≠ l√Ω\n",
        "contraction_mapping = {\n",
        "    \"ain't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"'cause\": \"because\",\n",
        "    \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",\n",
        "    \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
        "    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\n",
        "    \"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
        "    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "    \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\",\n",
        "    \"I'll've\": \"I will have\", \"I'm\": \"I am\", \"I've\": \"I have\",\n",
        "    \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",\n",
        "    \"i'll've\": \"i will have\", \"i'm\": \"i am\", \"i've\": \"i have\",\n",
        "    \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\",\n",
        "    \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\",\n",
        "    \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\",\n",
        "    \"might've\": \"might have\", \"mightn't\": \"might not\",\n",
        "    \"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "    \"mustn't\": \"must not\", \"mustn't've\": \"must not have\",\n",
        "    \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
        "    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\",\n",
        "    \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
        "    \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "    \"she'd\": \"she would\", \"she'd've\": \"she would have\",\n",
        "    \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "    \"should've\": \"should have\", \"shouldn't\": \"should not\",\n",
        "    \"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so as\",\n",
        "    \"this's\": \"this is\", \"that'd\": \"that would\", \"that'd've\": \"that would have\",\n",
        "    \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\",\n",
        "    \"there's\": \"there is\", \"here's\": \"here is\", \"they'd\": \"they would\",\n",
        "    \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n",
        "    \"they'll've\": \"they will have\", \"they're\": \"they are\",\n",
        "    \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\",\n",
        "    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\",\n",
        "    \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n",
        "    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\",\n",
        "    \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\",\n",
        "    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\",\n",
        "    \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\",\n",
        "    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "    \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\",\n",
        "    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\",\n",
        "    \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "    \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\",\n",
        "    \"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\",\n",
        "    \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "    \"you're\": \"you are\", \"you've\": \"you have\", \"u.s\": \"america\", \"e.g\": \"for example\"\n",
        "}\n",
        "\n",
        "punct_mapping = {\n",
        "    \"‚Äò\": \"'\", \"‚Çπ\": \"e\", \"¬¥\": \"'\", \"¬∞\": \"\", \"‚Ç¨\": \"e\", \"‚Ñ¢\": \"tm\", \"‚àö\": \" sqrt \",\n",
        "    \"√ó\": \"x\", \"¬≤\": \"2\", \"‚Äî\": \"-\", \"‚Äì\": \"-\", \"‚Äô\": \"'\", \"_\": \"-\", \"`\": \"'\", '‚Äú': '\"',\n",
        "    '‚Äù': '\"', '‚Äú': '\"', \"¬£\": \"e\", '‚àû': 'infinity', 'Œ∏': 'theta', '√∑': '/', 'Œ±': 'alpha',\n",
        "    '‚Ä¢': '.', '√†': 'a', '‚àí': '-', 'Œ≤': 'beta', '‚àÖ': '', '¬≥': '3', 'œÄ': 'pi'\n",
        "}\n",
        "\n",
        "mispell_dict = {\n",
        "    'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling',\n",
        "    'counselling': 'counseling', 'theatre': 'theater', 'cancelled': \"canceled\", 'labour': 'labor',\n",
        "    'organisation': \"organization\", 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ',\n",
        "    'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do',\n",
        "    'whatare': 'what are', 'howcan': \"how can\", 'howmuch': 'how much', 'howmany': 'how many',\n",
        "    'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does',\n",
        "    'mastrubation': 'masturbation', 'mastrubate': 'masturbate', 'mastrubating': 'masturbating',\n",
        "    'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data',\n",
        "    '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend',\n",
        "    'airhostess': 'air hostess', 'whst': 'what', 'watsapp': 'whatsapp',\n",
        "    'demonitisation': 'demonetization', 'demonitization': 'demonetization',\n",
        "    'demonetisation': \"demonetization\", 'pissed': 'pissed'\n",
        "}\n",
        "\n",
        "punct_chars = list((set(string.punctuation) | {\n",
        "    \"‚Äô\", \"‚Äò\", \"‚Äì\", \"‚Äî\", \"~\", \"|\", \"‚Äú\", \"‚Äù\", \"‚Ä¶\", \"'\", \"`\", \"_\", \"‚Äú\"\n",
        "}) - set([\"#\", \"!\", \"?\"]))\n",
        "punct_chars.sort()\n",
        "punctuation = \"\".join(punct_chars)\n",
        "replace = re.compile(\"[%s]\" % re.escape(punctuation))\n",
        "\n",
        "# H√†m t·∫£i GloVe v√† emoji2vec\n",
        "def load_embeddings(glove_path, emoji2vec_path, vocab_size, embedding_dim, logger):\n",
        "    logger.info(f\"ƒêang t·∫£i GloVe t·ª´ {glove_path}\")\n",
        "    glove_embeddings = {}\n",
        "    try:\n",
        "        with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                values = line.split()\n",
        "                word = values[0]\n",
        "                vector = np.asarray(values[1:], dtype='float32')\n",
        "                glove_embeddings[word] = vector\n",
        "        logger.info(f\"ƒê√£ t·∫£i {len(glove_embeddings)} vector GloVe\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"L·ªói khi t·∫£i GloVe t·ª´ {glove_path}: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "    logger.info(f\"ƒêang t·∫£i emoji2vec t·ª´ {emoji2vec_path}\")\n",
        "    emoji2vec = None\n",
        "    try:\n",
        "        with open(emoji2vec_path, 'r', encoding='utf-8') as f:\n",
        "            first_line = f.readline()\n",
        "            if not first_line.strip():\n",
        "                raise ValueError(f\"T·ªáp emoji2vec {emoji2vec_path} r·ªóng\")\n",
        "        emoji2vec = gensim.models.KeyedVectors.load_word2vec_format(\n",
        "            emoji2vec_path, binary=False, unicode_errors='ignore'\n",
        "        )\n",
        "        logger.info(f\"ƒê√£ t·∫£i {len(emoji2vec.key_to_index)} vector emoji\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"L·ªói khi t·∫£i emoji2vec t·ª´ {emoji2vec_path}: {str(e)}\")\n",
        "        raise\n",
        "    return glove_embeddings, emoji2vec\n",
        "\n",
        "# H√†m t·∫°o ma tr·∫≠n embedding\n",
        "def create_embedding_matrix(tokenizer, glove_embeddings, emoji2vec, vocab_size, embedding_dim, logger):\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    glove_hits, emoji_hits, misses = 0, 0, 0\n",
        "    for word, idx in tokenizer.word_index.items():\n",
        "        if idx >= vocab_size:\n",
        "            continue\n",
        "        if emoji.is_emoji(word) and word in emoji2vec:\n",
        "            embedding_matrix[idx] = emoji2vec[word]\n",
        "            emoji_hits += 1\n",
        "        elif word in glove_embeddings:\n",
        "            embedding_matrix[idx] = glove_embeddings[word]\n",
        "            glove_hits += 1\n",
        "        else:\n",
        "            misses += 1\n",
        "    logger.info(f\"Ma tr·∫≠n embedding: {glove_hits} GloVe hits, {emoji_hits} emoji2vec hits, {misses} misses\")\n",
        "    return embedding_matrix\n",
        "\n",
        "# H√†m ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n\n",
        "def clean_text(text, logger):\n",
        "    if not isinstance(text, str) or not text:\n",
        "        return \"\"\n",
        "    logger.debug(f\"VƒÉn b·∫£n g·ªëc: {text}\")\n",
        "\n",
        "    # S·ª≠a l·ªói m√£ h√≥a k√Ω t·ª±\n",
        "    text = ftfy.fix_text(text)\n",
        "\n",
        "    # X·ª≠ l√Ω contractions\n",
        "    for contraction, full_form in contraction_mapping.items():\n",
        "        text = text.replace(contraction, full_form)\n",
        "\n",
        "    # X·ª≠ l√Ω k√Ω t·ª± ƒë·∫∑c bi·ªát\n",
        "    for p, replacement in punct_mapping.items():\n",
        "        text = text.replace(p, replacement)\n",
        "\n",
        "    # X·ª≠ l√Ω hashtag\n",
        "    def split_hashtag(match):\n",
        "        hashtag = match.group(0)[1:]\n",
        "        words = wordninja.split(hashtag)\n",
        "        return ' '.join(words)\n",
        "    text = re.sub(r\"#\\w+\", split_hashtag, text)\n",
        "\n",
        "    # Lo·∫°i b·ªè URL v√† mention\n",
        "    text = re.sub(r\"http\\S*|\\S*\\.com\\S*|\\S*www\\S*\", \" \", text)\n",
        "    text = re.sub(r\"\\s@\\S+\", \" \", text)\n",
        "\n",
        "    # Lo·∫°i b·ªè d·∫•u c√¢u\n",
        "    text = replace.sub(\" \", text)\n",
        "\n",
        "    # Chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng v√† s·ª≠a l·ªói ch√≠nh t·∫£\n",
        "    text = text.lower()\n",
        "    words = text.split()\n",
        "    words = [mispell_dict.get(word, word) for word in words]\n",
        "    text = ' '.join(words)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    logger.debug(f\"VƒÉn b·∫£n ƒë√£ x·ª≠ l√Ω: {text}\")\n",
        "    return text\n",
        "\n",
        "# L·ªõp x·ª≠ l√Ω d·ªØ li·ªáu GoEmotions\n",
        "class GoemotionsProcessor:\n",
        "    def __init__(self, args, logger):\n",
        "        self.args = args\n",
        "        self.logger = logger\n",
        "\n",
        "    def get_labels(self):\n",
        "        label_file = os.path.join(self.args.data_dir, self.args.label_file)\n",
        "        self.logger.info(f\"ƒêang ƒë·ªçc t·ªáp nh√£n t·∫°i: {label_file}\")\n",
        "        if not os.path.exists(label_file):\n",
        "            self.logger.warning(f\"Kh√¥ng t√¨m th·∫•y t·ªáp nh√£n t·∫°i {label_file}. S·ª≠ d·ª•ng nh√£n m·∫∑c ƒë·ªãnh GoEmotions.\")\n",
        "            labels = [\n",
        "                'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion',\n",
        "                'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment',\n",
        "                'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism',\n",
        "                'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral'\n",
        "            ]\n",
        "            self.logger.info(f\"S·ª≠ d·ª•ng {len(labels)} nh√£n m·∫∑c ƒë·ªãnh\")\n",
        "            return labels\n",
        "        try:\n",
        "            with open(label_file, \"r\", encoding=\"utf-8\") as f:\n",
        "                labels = [line.strip() for line in f if line.strip()]\n",
        "            if not labels:\n",
        "                self.logger.error(f\"T·ªáp nh√£n {label_file} r·ªóng\")\n",
        "                raise ValueError(f\"T·ªáp nh√£n {label_file} r·ªóng\")\n",
        "            self.logger.info(f\"ƒê√£ ƒë·ªçc {len(labels)} nh√£n t·ª´ {label_file}\")\n",
        "            return labels\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"L·ªói khi ƒë·ªçc t·ªáp nh√£n {label_file}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _read_file(self, input_file):\n",
        "        if not os.path.exists(input_file):\n",
        "            self.logger.error(f\"Kh√¥ng t√¨m th·∫•y t·ªáp d·ªØ li·ªáu t·∫°i {input_file}\")\n",
        "            raise FileNotFoundError(f\"Kh√¥ng t√¨m th·∫•y t·ªáp d·ªØ li·ªáu t·∫°i {input_file}\")\n",
        "        try:\n",
        "            df = pd.read_csv(input_file, sep='\\t', header=None, names=['text', 'labels', 'id'])\n",
        "            self.logger.info(f\"ƒê√£ ƒë·ªçc {len(df)} d√≤ng t·ª´ {input_file}\")\n",
        "            if df.empty:\n",
        "                self.logger.error(f\"T·ªáp d·ªØ li·ªáu {input_file} r·ªóng\")\n",
        "                raise ValueError(f\"T·ªáp d·ªØ li·ªáu {input_file} r·ªóng\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"L·ªói khi ƒë·ªçc t·ªáp {input_file}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _augment_data(self, texts, labels):\n",
        "        aug = naw.SynonymAug(aug_p=0.3)\n",
        "        augmented_texts, augmented_labels = [], []\n",
        "        for text, label in zip(texts, labels):\n",
        "            augmented_texts.append(text)\n",
        "            augmented_labels.append(label)\n",
        "            aug_text = aug.augment(text)[0]\n",
        "            augmented_texts.append(aug_text)\n",
        "            augmented_labels.append(label)\n",
        "        self.logger.info(f\"S·ªë m·∫´u sau tƒÉng c∆∞·ªùng: {len(augmented_texts)}\")\n",
        "        return augmented_texts, augmented_labels\n",
        "\n",
        "    def _balance_labels(self, examples, label_list_len, set_type):\n",
        "        if set_type != \"train\":\n",
        "            return examples\n",
        "        self.logger.info(\"C√¢n b·∫±ng nh√£n cho t·∫≠p hu·∫•n luy·ªán\")\n",
        "        label_counts = Counter()\n",
        "        for ex in examples:\n",
        "            label_counts.update(ex['labels'])\n",
        "        self.logger.info(f\"Ph√¢n b·ªë nh√£n ban ƒë·∫ßu: {dict(label_counts)}\")\n",
        "        counts = [count for count in label_counts.values() if count > 0]\n",
        "        target_count = min(int(np.median(counts) * 6.0), len(examples) // 2)\n",
        "        self.logger.info(f\"S·ªë l∆∞·ª£ng m·ª•c ti√™u m·ªói nh√£n: {target_count}\")\n",
        "        balanced_examples = []\n",
        "        for label in range(label_list_len):\n",
        "            samples_with_label = [ex for ex in examples if label in ex['labels']]\n",
        "            current_count = label_counts[label]\n",
        "            if current_count == 0:\n",
        "                continue\n",
        "            elif current_count < target_count:\n",
        "                samples_needed = target_count - current_count\n",
        "                oversampled = random.choices(samples_with_label, k=samples_needed)\n",
        "                balanced_examples.extend(oversampled)\n",
        "            else:\n",
        "                samples_to_keep = max(target_count, int(current_count * 0.5))\n",
        "                balanced_examples.extend(random.sample(samples_with_label, min(samples_to_keep, len(samples_with_label))))\n",
        "        balanced_examples.extend([ex for ex in examples if not any(label in ex['labels'] for label in range(label_list_len))])\n",
        "        random.shuffle(balanced_examples)\n",
        "        new_label_counts = Counter()\n",
        "        for ex in balanced_examples:\n",
        "            new_label_counts.update(ex['labels'])\n",
        "        self.logger.info(f\"Ph√¢n b·ªë nh√£n sau c√¢n b·∫±ng: {dict(new_label_counts)}\")\n",
        "        self.logger.info(f\"S·ªë m·∫´u ban ƒë·∫ßu: {len(examples)}, S·ªë m·∫´u sau c√¢n b·∫±ng: {len(balanced_examples)}\")\n",
        "        return balanced_examples\n",
        "\n",
        "    def get_examples(self, mode):\n",
        "        file_map = {\n",
        "            'train': self.args.train_file,\n",
        "            'dev': self.args.dev_file,\n",
        "            'test': self.args.test_file\n",
        "        }\n",
        "        file_to_read = file_map.get(mode)\n",
        "        if not file_to_read:\n",
        "            raise ValueError(\"Mode ph·∫£i l√† 'train', 'dev', ho·∫∑c 'test'\")\n",
        "        file_path = os.path.join(self.args.data_dir, file_to_read)\n",
        "        self.logger.info(f\"ƒêang ƒë·ªçc d·ªØ li·ªáu {mode} t·ª´ {file_path}\")\n",
        "        df = self._read_file(file_path)\n",
        "        return self._create_examples(df, mode)\n",
        "\n",
        "    def _create_examples(self, df, set_type):\n",
        "        examples = []\n",
        "        label_list_len = len(self.get_labels())\n",
        "        label_counts = Counter()\n",
        "        for i, row in df.iterrows():\n",
        "            guid = f\"{set_type}-{i}\"\n",
        "            raw_text = row['text']\n",
        "            label_str = str(row['labels'])\n",
        "            try:\n",
        "                label = [int(l) for l in label_str.split(',') if l.strip().isdigit()]\n",
        "                label = [l for l in label if 0 <= l < label_list_len]\n",
        "                if not label:\n",
        "                    self.logger.warning(f\"Kh√¥ng c√≥ nh√£n h·ª£p l·ªá t·∫°i d√≤ng {i}: {label_str}. B·ªè qua.\")\n",
        "                    continue\n",
        "                label_counts.update(label)\n",
        "            except (ValueError, IndexError) as e:\n",
        "                self.logger.warning(f\"Nh√£n kh√¥ng h·ª£p l·ªá t·∫°i d√≤ng {i}: {label_str}. B·ªè qua. L·ªói: {e}\")\n",
        "                continue\n",
        "            cleaned_text = clean_text(raw_text, self.logger)\n",
        "            examples.append({\n",
        "                'guid': guid,\n",
        "                'text': cleaned_text,\n",
        "                'labels': label\n",
        "            })\n",
        "        self.logger.info(f\"ƒê√£ t·∫°o {len(examples)} m·∫´u t·ª´ {set_type}\")\n",
        "        self.logger.info(f\"Ph√¢n b·ªë nh√£n cho {set_type}: {dict(label_counts)}\")\n",
        "        if not examples:\n",
        "            self.logger.error(f\"Kh√¥ng t·∫°o ƒë∆∞·ª£c m·∫´u t·ª´ {set_type}. Ki·ªÉm tra t·ªáp d·ªØ li·ªáu!\")\n",
        "            raise ValueError(f\"Kh√¥ng t·∫°o ƒë∆∞·ª£c m·∫´u t·ª´ {set_type}\")\n",
        "        examples = self._balance_labels(examples, label_list_len, set_type)\n",
        "        return examples\n",
        "\n",
        "# H√†m t·∫£i d·ªØ li·ªáu\n",
        "def load_data(args, logger):\n",
        "    processor = GoemotionsProcessor(args, logger)\n",
        "    label_list = processor.get_labels()\n",
        "    num_labels = len(label_list)\n",
        "    def load_and_cache(mode):\n",
        "        cached_file = os.path.join(args.data_dir, f\"cached_{mode}_data.pkl\")\n",
        "        if os.path.exists(cached_file):\n",
        "            logger.info(f\"ƒêang t·∫£i d·ªØ li·ªáu ƒë√£ cache t·ª´ {cached_file}\")\n",
        "            with open(cached_file, 'rb') as f:\n",
        "                data = pickle.load(f)\n",
        "            return data['texts'], data['labels']\n",
        "        logger.info(f\"ƒêang x·ª≠ l√Ω d·ªØ li·ªáu {mode}\")\n",
        "        examples = processor.get_examples(mode)\n",
        "        texts = [ex['text'] for ex in examples]\n",
        "        labels = [ex['labels'] for ex in examples]\n",
        "        if mode == 'train':\n",
        "            texts, labels = processor._augment_data(texts, labels)\n",
        "        with open(cached_file, 'wb') as f:\n",
        "            pickle.dump({'texts': texts, 'labels': labels}, f)\n",
        "        logger.info(f\"ƒê√£ cache d·ªØ li·ªáu {mode} v√†o {cached_file}\")\n",
        "        return texts, labels\n",
        "    try:\n",
        "        train_texts, train_labels = load_and_cache('train')\n",
        "        val_texts, val_labels = load_and_cache('dev')\n",
        "        test_texts, test_labels = load_and_cache('test')\n",
        "        logger.info(f\"S·ªë m·∫´u hu·∫•n luy·ªán: {len(train_texts)}, S·ªë m·∫´u x√°c th·ª±c: {len(val_texts)}, S·ªë m·∫´u ki·ªÉm tra: {len(test_texts)}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"L·ªói khi t·∫£i d·ªØ li·ªáu: {str(e)}\")\n",
        "        raise\n",
        "    return train_texts, val_texts, test_texts, train_labels, val_labels, test_labels, label_list\n",
        "\n",
        "# H√†m m√£ h√≥a nh√£n th√†nh d·∫°ng multi-hot\n",
        "def to_multi_hot(label_lists, num_labels):\n",
        "    m = np.zeros((len(label_lists), num_labels), dtype=np.int32)\n",
        "    for i, labs in enumerate(label_lists):\n",
        "        m[i, labs] = 1\n",
        "    return m\n",
        "\n",
        "# H√†m t√≠nh tr·ªçng s·ªë l·ªõp s·ª≠ d·ª•ng log\n",
        "def compute_class_weights(labels, num_labels, logger):\n",
        "    label_counts = np.zeros(num_labels)\n",
        "    for labs in labels:\n",
        "        for l in labs:\n",
        "            label_counts[l] += 1\n",
        "\n",
        "    epsilon = 1e-8\n",
        "    label_counts = np.maximum(label_counts, epsilon)\n",
        "\n",
        "    total_samples = len(labels)\n",
        "    class_weights = {}\n",
        "    median_count = np.median(label_counts)\n",
        "    for i in range(num_labels):\n",
        "        if label_counts[i] > 0:\n",
        "            class_weights[i] = np.log(total_samples / label_counts[i])\n",
        "            if label_counts[i] < median_count:\n",
        "                class_weights[i] *= 2.0\n",
        "            class_weights[i] = max(class_weights[i], 1.0)\n",
        "        else:\n",
        "            class_weights[i] = 1.0\n",
        "\n",
        "    weight_sum = sum(class_weights.values())\n",
        "    if weight_sum > 0:\n",
        "        scale_factor = num_labels / weight_sum\n",
        "        for i in range(num_labels):\n",
        "            class_weights[i] *= scale_factor\n",
        "\n",
        "    logger.info(f\"Ph√¢n b·ªë nh√£n: {label_counts}\")\n",
        "    logger.info(f\"Tr·ªçng s·ªë l·ªõp (s·ª≠ d·ª•ng log): {class_weights}\")\n",
        "    return class_weights\n",
        "\n",
        "# H√†m t·ªëi ∆∞u h√≥a ng∆∞·ª°ng\n",
        "def optimize_threshold(y_true, y_pred_probs, logger):\n",
        "    best_threshold = 0.5\n",
        "    best_macro_f1 = 0.0\n",
        "    thresholds = np.arange(0.1, 0.91, 0.05)\n",
        "    for threshold in thresholds:\n",
        "        y_pred = (y_pred_probs >= threshold).astype(int)\n",
        "        macro_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "        if macro_f1 > best_macro_f1:\n",
        "            best_macro_f1 = macro_f1\n",
        "            best_threshold = threshold\n",
        "    logger.info(f\"Ng∆∞·ª°ng t·ªët nh·∫•t: {best_threshold}, Macro F1: {best_macro_f1:.4f}\")\n",
        "    return best_threshold\n",
        "\n",
        "# H√†m t·∫°o t·∫•t c·∫£ ma tr·∫≠n nh·∫ßm l·∫´n trong m·ªôt h√¨nh (subplots)\n",
        "def plot_confusion_matrices(y_true, y_pred, label_list, output_dir, model_type, logger):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    num_labels = len(label_list)\n",
        "    rows, cols = 5, 6  # L∆∞·ªõi 5x6 cho 28 nh√£n\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 3, rows * 2.5), constrained_layout=True)\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, label in enumerate(label_list):\n",
        "        cm = confusion_matrix(y_true[:, i], y_pred[:, i])\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'],\n",
        "                    yticklabels=['Negative', 'Positive'], ax=axes[i], cbar=False)\n",
        "        axes[i].set_title(f'{label}', fontsize=10)\n",
        "        axes[i].set_xlabel('Predicted', fontsize=8)\n",
        "        axes[i].set_ylabel('True', fontsize=8)\n",
        "        axes[i].tick_params(labelsize=8)\n",
        "\n",
        "    for j in range(len(label_list), len(axes)):\n",
        "        axes[j].axis('off')\n",
        "\n",
        "    cm_file = os.path.join(output_dir, f'all_confusion_matrices_{timestamp}.png')\n",
        "    plt.savefig(cm_file, bbox_inches='tight', dpi=300)\n",
        "    plt.close()\n",
        "    logger.info(f\"ƒê√£ l∆∞u t·∫•t c·∫£ ma tr·∫≠n nh·∫ßm l·∫´n cho {model_type} v√†o {cm_file}\")\n",
        "\n",
        "# H√†m t·∫°o ma tr·∫≠n nh·∫ßm l·∫´n t·ªïng h·ª£p\n",
        "def plot_aggregated_confusion_matrix(y_true, y_pred, output_dir, model_type, logger):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    num_labels = y_true.shape[1]\n",
        "    aggregated_cm = np.zeros((2, 2), dtype=int)\n",
        "\n",
        "    for i in range(num_labels):\n",
        "        cm = confusion_matrix(y_true[:, i], y_pred[:, i])\n",
        "        aggregated_cm += cm\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(aggregated_cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'],\n",
        "                yticklabels=['Negative', 'Positive'])\n",
        "    plt.title(f'Aggregated Confusion Matrix Across All Labels ({model_type})')\n",
        "    plt.ylabel('True')\n",
        "    plt.xlabel('Predicted')\n",
        "    cm_file = os.path.join(output_dir, f'aggregated_confusion_matrix_{timestamp}.png')\n",
        "    plt.savefig(cm_file, bbox_inches='tight', dpi=300)\n",
        "    plt.close()\n",
        "    logger.info(f\"ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n t·ªïng h·ª£p cho {model_type} v√†o {cm_file}\")\n",
        "\n",
        "# H√†m t·∫°o heatmap c·ªßa c√°c ch·ªâ s·ªë hi·ªáu su·∫•t\n",
        "def plot_performance_metrics_heatmap(y_true, y_pred, label_list, output_dir, model_type, logger):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=None, zero_division=0)\n",
        "    metrics = np.array([precision, recall, f1]).T  # Ma tr·∫≠n: h√†ng l√† nh√£n, c·ªôt l√† [precision, recall, f1]\n",
        "\n",
        "    plt.figure(figsize=(10, 12))\n",
        "    sns.heatmap(metrics, annot=True, fmt='.3f', cmap='YlGnBu', xticklabels=['Precision', 'Recall', 'F1-Score'],\n",
        "                yticklabels=label_list)\n",
        "    plt.title(f'Performance Metrics Heatmap ({model_type})')\n",
        "    heatmap_file = os.path.join(output_dir, f'performance_metrics_heatmap_{timestamp}.png')\n",
        "    plt.savefig(heatmap_file, bbox_inches='tight', dpi=300)\n",
        "    plt.close()\n",
        "    logger.info(f\"ƒê√£ l∆∞u heatmap ch·ªâ s·ªë hi·ªáu su·∫•t cho {model_type} v√†o {heatmap_file}\")\n",
        "\n",
        "# H√†m t·∫°o bi·ªÉu ƒë·ªì loss\n",
        "def plot_loss(history, output_dir, model_type, logger):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    epochs = range(1, len(history.history['loss']) + 1)\n",
        "\n",
        "    plt.plot(epochs, history.history['loss'], label='Training Loss', color='blue', linewidth=2)\n",
        "    plt.plot(epochs, history.history['val_loss'], label='Validation Loss', color='orange', linewidth=2)\n",
        "\n",
        "    plt.title(f'Training and Validation Loss for {model_type}')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    loss_plot_file = os.path.join(output_dir, f'loss_plot_{timestamp}.png')\n",
        "    plt.savefig(loss_plot_file, bbox_inches='tight', dpi=300)\n",
        "    plt.close()\n",
        "    logger.info(f\"ƒê√£ l∆∞u bi·ªÉu ƒë·ªì loss cho {model_type} v√†o {loss_plot_file}\")\n",
        "\n",
        "# H√†m d·ª± ƒëo√°n cho c√°c v√≠ d·ª• m·ªõi\n",
        "def predict_pipeline(model, tokenizer, texts, label_list, max_len, threshold=0.25, logger=None):\n",
        "    cleaned_texts = [clean_text(text, logger) for text in texts]\n",
        "    sequences = tokenizer.texts_to_sequences(cleaned_texts)\n",
        "    padded = pad_sequences(sequences, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
        "    pred_probs = model.predict(padded, verbose=0)\n",
        "    predictions = (pred_probs >= threshold).astype(int)\n",
        "    results = []\n",
        "    for i, text in enumerate(texts):\n",
        "        labels = [label_list[j] for j in range(len(label_list)) if predictions[i][j] == 1]\n",
        "        top_indices = np.argsort(pred_probs[i])[-3:][::-1]\n",
        "        top_labels = [label_list[j] for j in top_indices]\n",
        "        top_probs = [pred_probs[i][j] for j in top_indices]\n",
        "        results.append({\n",
        "            \"text\": text,\n",
        "            \"labels\": labels if labels else \"Kh√¥ng c√≥ nh√£n\",\n",
        "            \"top_labels\": list(zip(top_labels, top_probs)),\n",
        "            \"probs\": pred_probs[i].tolist()\n",
        "        })\n",
        "        print(f\"\\nV√≠ d·ª• {i+1}: {text}\")\n",
        "        print(f\"D·ª± ƒëo√°n (ng∆∞·ª°ng {threshold}): {labels if labels else 'Kh√¥ng c√≥ nh√£n'}\")\n",
        "        print(f\"Top-3 nh√£n: {list(zip(top_labels, top_probs))}\")\n",
        "    return results\n",
        "\n",
        "# H√†m ch√≠nh\n",
        "def main():\n",
        "    # C·∫•u h√¨nh tham s·ªë\n",
        "    parser = argparse.ArgumentParser(description=\"BiLSTM Multi-Label Classification for GoEmotions\")\n",
        "    parser.add_argument(\"--data_dir\", default=\"/content/drive/MyDrive/Goemotions/data\", type=str, help=\"Th∆∞ m·ª•c ch·ª©a d·ªØ li·ªáu\")\n",
        "    parser.add_argument(\"--train_file\", default=\"train.tsv\", type=str, help=\"T·ªáp d·ªØ li·ªáu hu·∫•n luy·ªán\")\n",
        "    parser.add_argument(\"--dev_file\", default=\"dev.tsv\", type=str, help=\"T·ªáp d·ªØ li·ªáu x√°c th·ª±c\")\n",
        "    parser.add_argument(\"--test_file\", default=\"test.tsv\", type=str, help=\"T·ªáp d·ªØ li·ªáu ki·ªÉm tra\")\n",
        "    parser.add_argument(\"--label_file\", default=\"labels.txt\", type=str, help=\"T·ªáp ch·ª©a danh s√°ch nh√£n\")\n",
        "    parser.add_argument(\"--glove_path\", default=\"/content/drive/MyDrive/Goemotions/glove.6B.300d.txt\", type=str, help=\"ƒê∆∞·ªùng d·∫´n ƒë·∫øn t·ªáp GloVe\")\n",
        "    parser.add_argument(\"--emoji2vec_path\", default=\"/content/drive/MyDrive/Goemotions/emoji2vec.txt\", type=str, help=\"ƒê∆∞·ªùng d·∫´n ƒë·∫øn t·ªáp emoji2vec\")\n",
        "    parser.add_argument(\"--ckpt_dir\", default=\"/content/drive/MyDrive/Goemotions/checkpoints\", type=str, help=\"Th∆∞ m·ª•c l∆∞u checkpoint m√¥ h√¨nh\")\n",
        "    parser.add_argument(\"--model_type\", default=\"goemotions-bilstm\", type=str, help=\"Lo·∫°i m√¥ h√¨nh (goemotions-bilstm ho·∫∑c vaafi-bilstm)\")\n",
        "\n",
        "    args = parser.parse_args([arg for arg in sys.argv[1:] if not arg.startswith('-f') and not arg.endswith('.json')])\n",
        "\n",
        "    # Mount Google Drive\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        print(\"Google Drive mounted successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error mounting Google Drive: {e}\")\n",
        "\n",
        "    # T·∫°o th∆∞ m·ª•c checkpoint\n",
        "    os.makedirs(args.ckpt_dir, exist_ok=True)\n",
        "    logger.info(f\"Th∆∞ m·ª•c checkpoint: {args.ckpt_dir}\")\n",
        "\n",
        "    # T·∫°o th∆∞ m·ª•c visualization v·ªõi run_id\n",
        "    run_id = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    output_dir = os.path.join(\"/content/drive/MyDrive/Goemotions/visualizations\", f\"{args.model_type}_{run_id}\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    logger.info(f\"Th∆∞ m·ª•c visualization: {output_dir}\")\n",
        "\n",
        "    # K√≠ch ho·∫°t mixed precision\n",
        "    set_global_policy('mixed_float16')\n",
        "\n",
        "    # T·∫£i d·ªØ li·ªáu\n",
        "    train_texts, val_texts, test_texts, train_labels, val_labels, test_labels, label_list = load_data(args, logger)\n",
        "    num_labels = len(label_list)\n",
        "\n",
        "    # M√£ h√≥a nh√£n\n",
        "    y_train = to_multi_hot(train_labels, num_labels)\n",
        "    y_val = to_multi_hot(val_labels, num_labels)\n",
        "    y_test = to_multi_hot(test_labels, num_labels)\n",
        "\n",
        "    # Token h√≥a v√† padding\n",
        "    vocab_size = 30000\n",
        "    max_len = 100\n",
        "    embedding_dim = 300\n",
        "\n",
        "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\", filters='!\"$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "    tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "    with open(os.path.join(args.ckpt_dir, f\"{args.model_type}_tokenizer.pkl\"), \"wb\") as f:\n",
        "        pickle.dump(tokenizer, f)\n",
        "    logger.info(f\"ƒê√£ l∆∞u tokenizer v√†o {args.model_type}_tokenizer.pkl\")\n",
        "\n",
        "    def encode(texts):\n",
        "        seq = tokenizer.texts_to_sequences(texts)\n",
        "        return pad_sequences(seq, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
        "\n",
        "    X_train = encode(train_texts)\n",
        "    X_val = encode(val_texts)\n",
        "    X_test = encode(test_texts)\n",
        "\n",
        "    # T·∫£i v√† t·∫°o ma tr·∫≠n embedding\n",
        "    glove_embeddings, emoji2vec = load_embeddings(args.glove_path, args.emoji2vec_path, vocab_size, embedding_dim, logger)\n",
        "    embedding_matrix = create_embedding_matrix(tokenizer, glove_embeddings, emoji2vec, vocab_size, embedding_dim, logger)\n",
        "\n",
        "    # Thi·∫øt l·∫≠p TPU ho·∫∑c GPU\n",
        "    try:\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "        tf.config.experimental_connect_to_cluster(tpu)\n",
        "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "        strategy = tf.distribute.TPUStrategy(tpu)\n",
        "        logger.info(\"Ch·∫°y tr√™n TPU\")\n",
        "    except ValueError:\n",
        "        strategy = tf.distribute.get_strategy()\n",
        "        logger.info(\"Ch·∫°y tr√™n GPU/CPU\")\n",
        "\n",
        "    with strategy.scope():\n",
        "        # X√¢y d·ª±ng m√¥ h√¨nh BiLSTM\n",
        "        inputs = Input(shape=(max_len,), dtype=\"int32\")\n",
        "        embed = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], trainable=True)(inputs)\n",
        "\n",
        "        # L·ªõp BiLSTM ƒë·∫ßu ti√™n\n",
        "        bilstm_out = Bidirectional(LSTM(128, return_sequences=True))(embed)\n",
        "        bilstm_out = Dropout(0.5)(bilstm_out)\n",
        "        # L·ªõp BiLSTM th·ª© hai\n",
        "        bilstm_out = Bidirectional(LSTM(64))(bilstm_out)\n",
        "        drop = Dropout(0.5)(bilstm_out)\n",
        "        output = Dense(num_labels, activation=\"sigmoid\")(drop)\n",
        "\n",
        "        model = Model(inputs, output)\n",
        "\n",
        "        # Bi√™n d·ªãch m√¥ h√¨nh\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=5e-4),\n",
        "            loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "            metrics=[\"accuracy\"]\n",
        "        )\n",
        "\n",
        "    # Thi·∫øt l·∫≠p callbacks (kh√¥ng c√≥ EarlyStopping)\n",
        "    checkpoint_path = os.path.join(args.ckpt_dir, f\"{args.model_type}_model_{{epoch:02d}}_{{val_accuracy:.4f}}.keras\")\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        checkpoint_path,\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        mode='max',\n",
        "        verbose=1\n",
        "    )\n",
        "    reduce_lr = ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.2,\n",
        "        patience=2,\n",
        "        min_lr=1e-6\n",
        "    )\n",
        "\n",
        "    # Hu·∫•n luy·ªán\n",
        "    batch_size = 128\n",
        "    epochs = 30\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        class_weight=compute_class_weights(train_labels, num_labels, logger),\n",
        "        callbacks=[checkpoint, reduce_lr]\n",
        "    )\n",
        "\n",
        "    # ƒê√°nh gi√°\n",
        "    loss, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
        "    logger.info(f\"Test Loss: {loss:.4f}, Test Accuracy: {acc:.4f}\")\n",
        "\n",
        "    # T·ªëi ∆∞u h√≥a ng∆∞·ª°ng tr√™n t·∫≠p validation\n",
        "    val_pred_probs = model.predict(X_val, batch_size=batch_size)\n",
        "    best_threshold = optimize_threshold(y_val, val_pred_probs, logger)\n",
        "    pred_probs = model.predict(X_test, batch_size=batch_size)\n",
        "    y_pred = (pred_probs >= best_threshold).astype(int)\n",
        "\n",
        "    # B√°o c√°o k·∫øt qu·∫£\n",
        "    raw_acc = accuracy_score(y_test.flatten(), y_pred.flatten())\n",
        "    micro_f1 = f1_score(y_test, y_pred, average='micro', zero_division=0)\n",
        "    macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "    weighted_f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    report = classification_report(y_test, y_pred, target_names=label_list, zero_division=0, digits=4)\n",
        "\n",
        "    logger.info(f\"\\nTest Loss: {loss:.4f}\")\n",
        "    logger.info(f\"Keras Accuracy (element-wise): {acc:.4f}\")\n",
        "    logger.info(f\"Raw Accuracy (sklearn, element-wise): {raw_acc:.4f}\")\n",
        "    logger.info(f\"Micro-averaged F1 score: {micro_f1:.4f}\")\n",
        "    logger.info(f\"Macro-averaged F1 score: {macro_f1:.4f}\")\n",
        "    logger.info(f\"Weighted-averaged F1 score: {weighted_f1:.4f}\")\n",
        "    logger.info(f\"\\nClassification Report (threshold {best_threshold}):\\n\" + report)\n",
        "\n",
        "    # T·∫°o v√† l∆∞u c√°c bi·ªÉu ƒë·ªì\n",
        "    plot_confusion_matrices(y_test, y_pred, label_list, output_dir, args.model_type, logger)\n",
        "    plot_aggregated_confusion_matrix(y_test, y_pred, output_dir, args.model_type, logger)\n",
        "    plot_performance_metrics_heatmap(y_test, y_pred, label_list, output_dir, args.model_type, logger)\n",
        "    plot_loss(history, output_dir, args.model_type, logger)\n",
        "\n",
        "    # Ph√¢n t√≠ch l·ªói\n",
        "    errors = []\n",
        "    for i, (true, pred, text) in enumerate(zip(y_test, y_pred, test_texts)):\n",
        "        if not np.array_equal(true, pred):\n",
        "            errors.append((text, true, pred))\n",
        "    logger.info(f\"C√°c m·∫´u d·ª± ƒëo√°n sai (top 10): {errors[:10]}\")\n",
        "\n",
        "    # D·ª± ƒëo√°n tr√™n v√≠ d·ª• m·ªõi\n",
        "    test_examples = [\n",
        "        \"Feeling on top of the world today! üéâüòä #BestDayEver #SoHappy\",\n",
        "        \"Totally let down... üò¢üíî #Disappointed #WhyThis\",\n",
        "        \"Omg that‚Äôs incredible news! üòç‚ú® #Amazing #Grateful\",\n",
        "        \"Head full of thoughts rn ü§Øü§î #Confused #Overthinking\",\n",
        "        \"Still can‚Äôt believe this happened. So pissed! üò°üî• #Angry #Unbelievable\"\n",
        "    ]\n",
        "    results = predict_pipeline(model, tokenizer, test_examples, label_list, max_len, best_threshold, logger)\n",
        "\n",
        "    # L∆∞u k·∫øt qu·∫£\n",
        "    output_file = os.path.join(output_dir, f\"BiLSTM-results_{run_id}.txt\")\n",
        "    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"\\n=== BiLSTM Results - {args.model_type} - {pd.Timestamp.now()} ===\\n\")\n",
        "        f.write(f\"Test Loss: {loss:.4f}\\n\")\n",
        "        f.write(f\"Keras Accuracy (element-wise): {acc:.4f}\\n\")\n",
        "        f.write(f\"Raw Accuracy (sklearn, element-wise): {raw_acc:.4f}\\n\")\n",
        "        f.write(f\"Micro-averaged F1 score: {micro_f1:.4f}\\n\")\n",
        "        f.write(f\"Macro-averaged F1 score: {macro_f1:.4f}\\n\")\n",
        "        f.write(f\"Weighted-averaged F1 score: {weighted_f1:.4f}\\n\")\n",
        "        f.write(f\"\\nClassification Report (threshold {best_threshold}):\\n\" + report + \"\\n\")\n",
        "        f.write(\"\\nExample Predictions:\\n\")\n",
        "        for i, result in enumerate(results):\n",
        "            f.write(f\"Example {i+1}: {result['text']}\\n\")\n",
        "            f.write(f\"Predicted Labels: {result['labels']}\\n\")\n",
        "            f.write(f\"Top-3 Labels and Probabilities: {result['top_labels']}\\n\\n\")\n",
        "        f.write(\"================================\\n\")\n",
        "\n",
        "    # L∆∞u m√¥ h√¨nh v√† tokenizer\n",
        "    logger.info(f\"ƒêang l∆∞u m√¥ h√¨nh v√† tokenizer\")\n",
        "    local_model_dir = os.path.join(args.ckpt_dir, f\"{args.model_type}\")\n",
        "    os.makedirs(local_model_dir, exist_ok=True)\n",
        "    model.save(os.path.join(local_model_dir, f\"{args.model_type}_model.keras\"))\n",
        "    with open(os.path.join(local_model_dir, f\"{args.model_type}_tokenizer.pkl\"), \"wb\") as f:\n",
        "        pickle.dump(tokenizer, f)\n",
        "\n",
        "    with open(os.path.join(local_model_dir, f\"{args.model_type}_labels.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        for label in label_list:\n",
        "            f.write(f\"{label}\\n\")\n",
        "\n",
        "    config = {\n",
        "        \"vocab_size\": vocab_size,\n",
        "        \"max_len\": max_len,\n",
        "        \"embedding_dim\": embedding_dim,\n",
        "        \"num_labels\": num_labels,\n",
        "        \"bilstm_units\": [128, 64],\n",
        "        \"dropout_rate\": 0.5,\n",
        "        \"learning_rate\": 5e-4,\n",
        "        \"best_threshold\": best_threshold,\n",
        "        \"model_type\": args.model_type\n",
        "    }\n",
        "    with open(os.path.join(local_model_dir, f\"{args.model_type}_config.json\"), \"w\") as f:\n",
        "        import json\n",
        "        json.dump(config, f, indent=4)\n",
        "\n",
        "    logger.info(f\"ƒê√£ l∆∞u m√¥ h√¨nh v√† tokenizer t·∫°i: {local_model_dir}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "M4uxniP7YZ9Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}